import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO
import sys
import os
from openpyxl.styles import PatternFill

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆè¡¥å……æ°´å°å…³é”®è¯+å¼ºåŒ–åŒ¹é…ï¼‰ ----------------------
# 1. æ–°å¢â€œä¾â€â€œä¾å…°â€ç­‰æ°´å°å…³é”®è¯ï¼Œå½»åº•æ¸…ç†æ®‹ç•™
REDUNDANT_KEYWORDS = [
    "å†…éƒ¨ä½¿ç”¨", "CONFIDENTIAL", "è‰ç¨¿", "ç°è´§è¯•ç»“ç®—æœŸé—´", "æ—¥æ¸…åˆ†å•æ®",
    "å…¬å¸åç§°", "ç¼–å·ï¼š", "å•ä½ï¼š", "æ¸…åˆ†æ—¥æœŸ", "åˆè®¡ç”µé‡", "åˆè®¡ç”µè´¹",
    "è®¡é‡ç”µé‡", "ç”µèƒ½é‡ç”µè´¹", "å®¡æ‰¹ï¼š", "å®¡æ ¸ï¼š", "ç¼–åˆ¶ï¼š", "åŠ ç›–ç”µå­ç­¾ç« ",
    "ylxxhfd", "yxxchfd", "ä¾å…°å¿ååˆé£åŠ›å‘ç”µæœ‰é™å…¬å¸", "ä¾", "ä¾å…°", "ååˆ",  # æ–°å¢æ°´å°å…³é”®è¯
    "å¿", "é£åŠ›å‘ç”µ", "æœ‰é™å…¬å¸"  # æ–°å¢åœºç«™åç§°å†—ä½™å­—ç¬¦
]
TRADE_CODE_MAP = {
    "0101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",  # ä¼˜å…ˆå‘ç”µäº¤æ˜“ç¼–ç ç½®é¡¶ï¼Œç¡®ä¿ä¼˜å…ˆåŒ¹é…
    "0101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "0101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "0101040203": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040301": "é€è¾½å®äº¤æ˜“",
    "0101040321": "é€ååŒ—äº¤æ˜“", 
    "0101040322": "é€å±±ä¸œäº¤æ˜“",
    "0101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "0102020101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "0102020301": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "0102010101": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "0102010201": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "0202030001": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "0202030002": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "0101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "0101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "0101100101": "åå·®è€ƒæ ¸è´¹ç”¨",
    "0101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101100001": "æ—¥èåˆäº¤æ˜“",
    "101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",  # è¡¥å……ä¼˜å…ˆå‘ç”µ9ä½ç¼–ç 
    "101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101100001": "æ—¥èåˆäº¤æ˜“",
    "101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "101100101": "åå·®è€ƒæ ¸è´¹ç”¨"
}
# 2. å…³é”®è¯åŒ¹é…é¡ºåºè°ƒæ•´ï¼šâ€œä¼˜å…ˆå‘ç”µâ€ç½®é¡¶ï¼Œç¡®ä¿ç¬¬ä¸€æ—¶é—´è¯†åˆ«ï¼ˆä¿®å¤ï¼šæ”¹ä¸ºå­—å…¸{}ï¼Œè€Œéåˆ—è¡¨[]ï¼‰
TRADE_KEYWORDS = {
    "ä¼˜å…ˆå‘ç”µ": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",  # ä¼˜å…ˆå‘ç”µå…³é”®è¯ç½®é¡¶
    "ä»£ç†è´­ç”µ": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“",
    "ç›´æ¥äº¤æ˜“": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "ç»¿è‰²ç”µåŠ›": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æ±Ÿè‹": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æµ™æ±Ÿ": "é€æµ™æ±Ÿäº¤æ˜“",
    "é€æµ™æ±Ÿçœé—´": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€ä¸Šæµ·": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€è¾½å®": "é€è¾½å®äº¤æ˜“",
    "é€ååŒ—": "é€ååŒ—äº¤æ˜“",
    "é€å±±ä¸œ": "é€å±±ä¸œäº¤æ˜“",
    "æ—¥èåˆ": "æ—¥èåˆäº¤æ˜“",
    "ç°è´§æ—¥å‰": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "ç°è´§å®æ—¶": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "é˜»å¡è´¹ç”¨": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "ä»·å·®è´¹ç”¨": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "ç°è´§ç»“ç®—": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "è¾…åŠ©æœåŠ¡": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "åå·®è€ƒæ ¸": "åå·®è€ƒæ ¸è´¹ç”¨"
}
DATA_RULES = {
    "ç”µé‡(å…†ç“¦æ—¶)": {"min": -1000, "max": 5000},
    "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": {"min": 0, "max": 2000},
    "ç”µè´¹(å…ƒ)": {"min": -10000, "max": 10000000}
}
STATION_SPLIT_KEYWORDS = ["æœºç»„", "åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", "Aé£ç”µåœº", "Bé£ç”µåœº"]
# 3. åœºç«™åç§°æ ¸å¿ƒå…³é”®è¯ï¼ˆç”¨äºæœ€ç»ˆæ¸…ç†ï¼‰
STATION_CORE_NAMES = ["åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", "æ™¶ç››å…‰ä¼ç”µç«™"]

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆå…¨é“¾è·¯ä¿®å¤ï¼‰ ----------------------
def remove_redundant_text(text):
    """å¼ºåŒ–æ°´å°æ¸…ç†ï¼šå¤šæ¬¡æ¸…ç†ç¡®ä¿æ— æ®‹ç•™"""
    if not text:
        return ""
    cleaned = str(text).strip()
    # 1. é¦–æ¬¡æ¸…ç†å†—ä½™å…³é”®è¯
    for keyword in REDUNDANT_KEYWORDS:
        cleaned = cleaned.replace(keyword, "")
    # 2. äºŒæ¬¡æ¸…ç†ï¼šç§»é™¤å•ä¸ªæ°´å°å­—ç¬¦ï¼ˆå¦‚â€œä¾â€â€œåâ€ï¼‰
    single_watermarks = ["ä¾", "å…°", "å", "åˆ", "å¿", "ç”µ", "åŠ›", "å‘", "é™"]
    for char in single_watermarks:
        cleaned = cleaned.replace(char, "")
    # 3. æ¸…ç†è¿ç»­ç©ºç™½ç¬¦å’Œä¹±ç 
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.\-\: ]', '', cleaned)
    return cleaned.strip()

def clean_station_name(station_name):
    """ä¿®å¤1ï¼šåœºç«™åç§°æœ€ç»ˆæ¸…ç†ï¼Œç¡®ä¿æ— æ°´å°æ®‹ç•™"""
    if not station_name:
        return "æœªçŸ¥åœºç«™"
    # 1. å…ˆæ¸…ç†å†—ä½™æ–‡æœ¬
    cleaned = remove_redundant_text(station_name)
    # 2. åŒ¹é…æ ¸å¿ƒåœºç«™åç§°ï¼ˆç²¾å‡†æå–â€œåŒå‘A/Bé£ç”µåœºâ€ï¼‰
    for core_name in STATION_CORE_NAMES:
        if core_name in cleaned:
            return core_name
    # 3. å…œåº•ï¼šæå–å«â€œé£ç”µåœº/å…‰ä¼ç”µç«™â€çš„éƒ¨åˆ†
    for type_key in ["é£ç”µåœº", "å…‰ä¼ç”µç«™", "ç”µç«™"]:
        if type_key in cleaned:
            match = re.search(r'([^ï¼Œã€‚\n]+' + type_key + ')', cleaned)
            if match:
                return match.group(1).strip()
    return cleaned

def safe_convert_to_numeric(value, data_type=""):
    if value is None or pd.isna(value) or value == '':
        return None
    val_str = remove_redundant_text(value)
    if re.match(r'^\d{9,10}$', val_str) or val_str in ['-', '.', '', 'â€”', 'â€”â€”']:
        return None
    try:
        cleaned = re.sub(r'[^\d\-\.]', '', val_str.replace('ï¼Œ', ',').replace('ã€‚', '.'))
        if not cleaned or cleaned in ['-', '.', '-.' , '-.']:
            return None
        num = float(cleaned)
        if data_type in DATA_RULES:
            rule = DATA_RULES[data_type]
            if num < rule["min"] or num > rule["max"]:
                return None
        return num
    except (ValueError, TypeError):
        return None

def extract_company_info(pdf_text, file_name):
    clean_text = remove_redundant_text(pdf_text)
    company_name = "æœªçŸ¥å‘ç”µå…¬å¸"
    company_match = re.search(r'å…¬å¸åç§°[:ï¼š]\s*([^ï¼Œã€‚\n]+å…¬å¸)', clean_text)
    if company_match:
        company_name = company_match.group(1).strip()
    else:
        company_match = re.search(r'([^_]+å…¬å¸|[^_]+å‘ç”µ)', file_name)
        if company_match:
            company_name = company_match.group(1).strip()
    return company_name

def extract_clear_date(pdf_text, file_name):
    clean_text = remove_redundant_text(pdf_text)
    date = None
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'ç»“ç®—æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'(\d{4}-\d{1,2}-\d{1,2})'
    ]
    for pattern in date_patterns:
        match = re.search(pattern, clean_text)
        if match:
            date_str = match.group(1).strip()
            if "å¹´" in date_str:
                date_str = date_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            date = date_str
            break
    if not date:
        date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2}|\d{8})', file_name)
        if date_match:
            date_str = date_match.group(1)
            if len(date_str) == 8:
                date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date = date_str
    return date

def split_double_station_tables(all_tables, pdf_text):
    """ä¿®å¤2ï¼šæ‹†åˆ†åœºç«™æ—¶å¼ºåŒ–åç§°æ¸…ç†"""
    clean_text = remove_redundant_text(pdf_text)
    merged_rows = []
    for table in all_tables:
        for row in table:
            cleaned_row = [remove_redundant_text(cell) for cell in row]
            if any(cell.strip() != "" for cell in cleaned_row):
                merged_rows.append(cleaned_row)
    
    if not merged_rows:
        return []
    
    station_segments = []
    current_segment = []
    current_station = "æœªçŸ¥åœºç«™"
    
    for row in merged_rows:
        row_str = ''.join(row).replace(" ", "")
        if any(keyword in row_str for keyword in STATION_SPLIT_KEYWORDS):
            if current_segment:
                # ä¿å­˜å‰ä¸€æ®µæ—¶æ¸…ç†åœºç«™åç§°
                cleaned_station = clean_station_name(current_station)
                station_segments.append((cleaned_station, current_segment))
            # æå–å½“å‰åœºç«™åç§°å¹¶æ¸…ç†
            current_station = "æœªçŸ¥åœºç«™"
            for cell in row:
                cell_clean = remove_redundant_text(cell)
                if any(keyword in cell_clean for keyword in ["åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", "Aé£ç”µåœº", "Bé£ç”µåœº"]):
                    current_station = cell_clean
                    break
            current_station = clean_station_name(current_station)  # å…³é”®ï¼šæ¸…ç†åœºç«™åç§°
            current_segment = [row]
        else:
            current_segment.append(row)
    
    # ä¿å­˜æœ€åä¸€æ®µ
    if current_segment:
        cleaned_station = clean_station_name(current_station)
        station_segments.append((cleaned_station, current_segment))
    
    valid_segments = [(station, seg) for station, seg in station_segments if len(seg) >= 3]
    return valid_segments

def get_trade_name(trade_code, trade_text):
    """ä¿®å¤3ï¼šä¼˜å…ˆåŒ¹é…ä¼˜å…ˆå‘ç”µäº¤æ˜“ï¼Œé¿å…æœªè¯†åˆ«"""
    # 1. ä¼˜å…ˆç¼–ç åŒ¹é…ï¼ˆä¼˜å…ˆå‘ç”µç¼–ç ç½®é¡¶ï¼Œä¼˜å…ˆåŒ¹é…ï¼‰
    if trade_code in TRADE_CODE_MAP:
        return TRADE_CODE_MAP[trade_code]
    # 2. å…³é”®è¯åŒ¹é…ï¼ˆä¼˜å…ˆå‘ç”µå…³é”®è¯ç½®é¡¶ï¼Œä¼˜å…ˆè¯†åˆ«ï¼‰
    for key, name in TRADE_KEYWORDS.items():
        if key in trade_text:
            return name
    # 3. æœªåŒ¹é…åˆ°åˆ™è¿”å›â€œæœªè¯†åˆ«ç§‘ç›®â€ï¼ˆåç»­ä¼šè¢«è¿‡æ»¤ï¼‰
    return "æœªè¯†åˆ«ç§‘ç›®"

def parse_single_station_data(station_name, table_segment, company_name, clear_date):
    """ä¿®å¤4ï¼šè¿‡æ»¤æ— æ•ˆè¡Œï¼Œç¡®ä¿ä¼˜å…ˆå‘ç”µäº¤æ˜“ä¸ºç¬¬ä¸€è¡Œ"""
    trade_records = []
    valid_rows = []
    
    # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ— æ•ˆè¡Œï¼ˆä»…ä¿ç•™å«æœ‰æ•ˆç¼–ç /ç§‘ç›®/æ•°æ®çš„è¡Œï¼‰
    for row in table_segment:
        row_clean = [remove_redundant_text(cell) for cell in row]
        row_str = ''.join(row_clean).replace(" ", "")
        is_empty = all(cell == '' for cell in row_clean)
        is_header = any(keyword in row_str for keyword in ["ç§‘ç›®ç¼–ç ", "ç»“ç®—ç±»å‹", "ç”µé‡", "ç”µä»·", "ç”µè´¹"])
        # æœ‰æ•ˆè¡Œæ¡ä»¶ï¼š1. å«10/9ä½ç¼–ç ï¼›2. å«æ˜ç¡®ç§‘ç›®å…³é”®è¯ï¼›3. å«æœ‰æ•ˆæ•°æ®ï¼›4. éç©ºéè¡¨å¤´
        has_code = any(re.match(r'^\d{9,10}$', cell.replace(" ", "")) for cell in row_clean)
        has_trade_key = any(key in row_str for key in TRADE_KEYWORDS.keys())
        has_valid_data = any(safe_convert_to_numeric(cell) is not None for cell in row_clean if cell not in ['', '-'])
        
        if (has_code or has_trade_key or has_valid_data) and not is_empty and not is_header:
            valid_rows.append(row_clean)
    
    if len(valid_rows) < 2:
        return trade_records
    
    # ç¬¬äºŒæ­¥ï¼šå®šä½åˆ—ï¼ˆç¡®ä¿ç»“ç®—ç±»å‹åˆ—å‡†ç¡®ï¼‰
    cols = {"code": -1, "name": -1, "qty": -1, "price": -1, "fee": -1}
    header_idx = -1
    for idx, row in enumerate(valid_rows[:3]):
        row_str = ''.join(row).replace(" ", "")
        if "ç»“ç®—ç±»å‹" in row_str:
            header_idx = idx
            break
    if header_idx == -1:
        header_idx = 0
    header_row = valid_rows[header_idx]
    
    for col_idx, cell in enumerate(header_row):
        cell_clean = remove_redundant_text(cell).lower()
        if "ç¼–ç " in cell_clean:
            cols["code"] = col_idx
        elif "ç»“ç®—ç±»å‹" in cell_clean:
            cols["name"] = col_idx
        elif "ç”µé‡" in cell_clean and "ä»·" not in cell_clean:
            cols["qty"] = col_idx
        elif "ç”µä»·" in cell_clean or "å•ä»·" in cell_clean:
            cols["price"] = col_idx
        elif "ç”µè´¹" in cell_clean or "é‡‘é¢" in cell_clean:
            cols["fee"] = col_idx
    
    if any(v == -1 for v in cols.values()) and len(header_row) >= 5:
        cols = {"code": 0, "name": 1, "qty": 2, "price": 3, "fee": 4}
    
    # ç¬¬ä¸‰æ­¥ï¼šè§£ææ•°æ®è¡Œï¼ˆåªä¿ç•™æœ‰æ•ˆç§‘ç›®ï¼Œå‰”é™¤æœªè¯†åˆ«ï¼‰
    data_start_idx = header_idx + 1
    for row_idx in range(data_start_idx, len(valid_rows)):
        row = valid_rows[row_idx]
        row_str = ''.join(row).replace(" ", "")
        is_subtotal = "å°è®¡" in row_str
        
        if "åˆè®¡" in row_str and not is_subtotal:
            continue
        
        # æå–ç¼–ç å’Œåç§°
        trade_code = row[cols["code"]].strip().replace(" ", "") if (cols["code"] < len(row)) else ""
        trade_text = row[cols["name"]].strip() if (cols["name"] < len(row)) else ""
        trade_name = get_trade_name(trade_code, trade_text)
        
        # å…³é”®è¿‡æ»¤ï¼šå‰”é™¤æœªè¯†åˆ«ç§‘ç›®ï¼ˆè§£å†³ç¬¬ä¸€ä¸ªç§‘ç›®æœªè¯†åˆ«é—®é¢˜ï¼‰
        if trade_name == "æœªè¯†åˆ«ç§‘ç›®" and not is_subtotal:
            continue
        
        # å¤„ç†å°è®¡è¡Œ
        if is_subtotal:
            subtotal_qty = None
            subtotal_fee = None
            nums = [safe_convert_to_numeric(cell, "ç”µé‡(å…†ç“¦æ—¶)") for cell in row if safe_convert_to_numeric(cell) is not None]
            fee_nums = [safe_convert_to_numeric(cell, "ç”µè´¹(å…ƒ)") for cell in row if safe_convert_to_numeric(cell) is not None]
            if nums:
                subtotal_qty = nums[0]
            if fee_nums:
                subtotal_fee = fee_nums[-1]
            trade_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "å½“æ—¥å°è®¡",
                "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
                "ç”µé‡(å…†ç“¦æ—¶)": subtotal_qty,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": subtotal_fee
            })
            continue
        
        # å¤„ç†æ™®é€šç§‘ç›®ï¼ˆä¼˜å…ˆå‘ç”µäº¤æ˜“å·²ä¼˜å…ˆè¯†åˆ«ï¼‰
        quantity = safe_convert_to_numeric(row[cols["qty"]], "ç”µé‡(å…†ç“¦æ—¶)") if (cols["qty"] < len(row)) else None
        price = safe_convert_to_numeric(row[cols["price"]], "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)") if (cols["price"] < len(row)) else None
        fee = safe_convert_to_numeric(row[cols["fee"]], "ç”µè´¹(å…ƒ)") if (cols["fee"] < len(row)) else None
        
        if "é˜»å¡è´¹ç”¨" in trade_name or "ä»·å·®è´¹ç”¨" in trade_name or "è¾…åŠ©æœåŠ¡" in trade_name or "åå·®è€ƒæ ¸" in trade_name:
            quantity = None
            price = None
        
        if quantity is None and fee is None:
            continue
        
        trade_records.append({
            "å…¬å¸åç§°": company_name,
            "åœºç«™åç§°": station_name,
            "æ¸…åˆ†æ—¥æœŸ": clear_date,
            "ç§‘ç›®åç§°": trade_name,
            "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
            "ç”µé‡(å…†ç“¦æ—¶)": quantity,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": price,
            "ç”µè´¹(å…ƒ)": fee
        })
    
    return trade_records

# ---------------------- ä¸»è§£æå‡½æ•° ----------------------
def parse_pdf_final(file_obj, file_name):
    try:
        file_obj.seek(0)
        file_bytes = BytesIO(file_obj.read())
        file_bytes.seek(0)
        
        all_text = ""
        all_tables = []
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                all_text += text + "\n"
                tables = page.extract_tables({
                    "vertical_strategy": "lines_strict",
                    "horizontal_strategy": "lines_strict",
                    "snap_tolerance": 1,
                    "join_tolerance": 1,
                    "edge_min_length": 3
                })
                all_tables.extend(tables)
        
        company_name = extract_company_info(all_text, file_name)
        clear_date = extract_clear_date(all_text, file_name)
        station_segments = split_double_station_tables(all_tables, all_text)
        
        if not station_segments:
            station_segments = [("æœªçŸ¥åœºç«™", [row for table in all_tables for row in table])]
        
        all_records = []
        for station_name, table_segment in station_segments:
            # å†æ¬¡æ¸…ç†åœºç«™åç§°ï¼Œç¡®ä¿æ— æ®‹ç•™
            cleaned_station = clean_station_name(station_name)
            station_data = parse_single_station_data(cleaned_station, table_segment, company_name, clear_date)
            all_records.extend(station_data)
        
        # å»é‡
        unique_records = []
        seen_keys = set()
        for rec in all_records:
            key = f"{rec['åœºç«™åç§°']}_{rec['ç§‘ç›®åç§°']}_{rec['åŸå§‹ç§‘ç›®ç¼–ç ']}"
            if key not in seen_keys:
                seen_keys.add(key)
                unique_records.append(rec)
        
        return unique_records if len(unique_records) > 0 else [{
            "å…¬å¸åç§°": "æœªçŸ¥å‘ç”µå…¬å¸",
            "åœºç«™åç§°": "æœªçŸ¥åœºç«™",
            "æ¸…åˆ†æ—¥æœŸ": None,
            "ç§‘ç›®åç§°": "è§£æå¤±è´¥",
            "åŸå§‹ç§‘ç›®ç¼–ç ": "",
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
            "ç”µé‡(å…†ç“¦æ—¶)": None,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
            "ç”µè´¹(å…ƒ)": None
        }]
    
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {str(e)}")
        return [{
            "å…¬å¸åç§°": "æœªçŸ¥å‘ç”µå…¬å¸",
            "åœºç«™åç§°": "æœªçŸ¥åœºç«™",
            "æ¸…åˆ†æ—¥æœŸ": None,
            "ç§‘ç›®åç§°": "è§£æå¤±è´¥",
            "åŸå§‹ç§‘ç›®ç¼–ç ": "",
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
            "ç”µé‡(å…†ç“¦æ—¶)": None,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
            "ç”µè´¹(å…ƒ)": None
        }]

# ---------------------- Streamlitåº”ç”¨ ----------------------
def main():
    st.set_page_config(page_title="é€šç”¨æ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰", layout="wide")
    
    st.title("ğŸ“Š é€šç”¨ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆåŒåœºç«™ç²¾å‡†ç‰ˆï¼‰")
    st.markdown("**æ ¸å¿ƒä¿®å¤ï¼šåœºç«™åç§°å»æ°´å° | ä¼˜å…ˆå‘ç”µäº¤æ˜“ç½®é¡¶ | æœªè¯†åˆ«ç§‘ç›®è¿‡æ»¤**")
    st.divider()
    
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆæ”¯æŒåŒåœºç«™/å¤šé¡µé¢ï¼‰",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸš€ å¼€å§‹æ‰¹é‡æå–", type="primary"):
        st.divider()
        st.subheader("âš™ï¸ å¤„ç†è¿›åº¦")
        
        all_results = []
        progress_bar = st.progress(0)
        
        for idx, file in enumerate(uploaded_files):
            st.write(f"æ­£åœ¨å¤„ç†ï¼š{file.name}")
            file_results = parse_pdf_final(file, file.name)
            all_results.extend(file_results)
            progress_bar.progress((idx + 1) / len(uploaded_files))
            file.close()
        
        progress_bar.empty()
        
        df = pd.DataFrame(all_results).fillna("")
        display_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "åŸå§‹ç§‘ç›®ç¼–ç ", "åŸå§‹ç§‘ç›®æ–‡æœ¬", "ç”µé‡(å…†ç“¦æ—¶)", 
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_display = df[[col for col in display_cols if col in df.columns]]
        
        st.subheader("ğŸ“ˆ æ‰¹é‡æå–ç»“æœï¼ˆåŒåœºç«™ç²¾å‡†æ‹†åˆ†ï¼‰")
        def highlight_rows(row):
            if row["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                return ["background-color: #e6f3ff"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Aé£ç”µåœº":
                return ["background-color: #f0fff4"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Bé£ç”µåœº":
                return ["background-color: #fff8f0"] * len(row)
            else:
                return [""] * len(row)
        styled_df = df_display.style.apply(highlight_rows, axis=1)
        st.dataframe(styled_df, use_container_width=True)
        
        total_stations = df["åœºç«™åç§°"].nunique()
        total_trades = len(df[df["ç§‘ç›®åç§°"] != "å½“æ—¥å°è®¡"])
        subtotal_count = len(df[df["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡"])
        st.info(f"**ç»Ÿè®¡ï¼š** è¦†ç›–åœºç«™ {total_stations} ä¸ª | æœ‰æ•ˆç§‘ç›® {total_trades} ä¸ª | å°è®¡è¡Œ {subtotal_count} ä¸ª")
        
        # ä¸‹è½½Excelï¼ˆå‰”é™¤åŸå§‹ç¼–ç /æ–‡æœ¬ï¼‰
        download_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "ç”µé‡(å…†ç“¦æ—¶)", "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_download = df[[col for col in download_cols if col in df.columns]]
        
        output = BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_download.to_excel(writer, index=False, sheet_name="å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®")
            ws = writer.sheets["å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®"]
            light_blue = PatternFill(start_color="E6F3FF", end_color="E6F3FF", fill_type="solid")
            for row in range(2, len(df_download) + 2):
                if df_download.iloc[row-2]["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                    for col in range(1, len(df_download.columns) + 1):
                        ws.cell(row=row, column=col).fill = light_blue
        
        output.seek(0)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excelï¼ˆä¸å«åŸå§‹ç¼–ç /æ–‡æœ¬ï¼‰",
            data=output,
            file_name=f"å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        st.success("âœ… æå–å®Œæˆï¼åŒå‘Bé£ç”µåœºåç§°æ— æ°´å°æ®‹ç•™ï¼ŒåŒå‘Aé£ç”µåœºä¼˜å…ˆå‘ç”µäº¤æ˜“ä¸ºç¬¬ä¸€è¡Œ")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ åŒåœºç«™ï¼ˆå¦‚åŒå‘A/Bé£ç”µåœºï¼‰çš„ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•PDF")

if __name__ == "__main__":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    main()
