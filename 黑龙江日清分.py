import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO
import sys
import os
from openpyxl.styles import PatternFill

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆå¼ºåŒ–è¾¹ç•Œé™åˆ¶ï¼‰ ----------------------
REDUNDANT_KEYWORDS = [
    "å†…éƒ¨ä½¿ç”¨", "CONFIDENTIAL", "è‰ç¨¿", "ç°è´§è¯•ç»“ç®—æœŸé—´", "æ—¥æ¸…åˆ†å•æ®",
    "å…¬å¸åç§°", "ç¼–å·ï¼š", "å•ä½ï¼š", "æ¸…åˆ†æ—¥æœŸ", "åˆè®¡ç”µé‡", "åˆè®¡ç”µè´¹",
    "ç”µèƒ½é‡ç”µè´¹", "å®¡æ‰¹ï¼š", "å®¡æ ¸ï¼š", "ç¼–åˆ¶ï¼š", "åŠ ç›–ç”µå­ç­¾ç« ",
    "ylxxhfd", "yxxchfd", "ä¾å…°å¿ååˆé£åŠ›å‘ç”µæœ‰é™å…¬å¸", "ä¾", "ä¾å…°", "ååˆ",
    "å¿", "é£åŠ›å‘ç”µ", "æœ‰é™å…¬å¸"
]
TRADE_CODE_MAP = {
    "0101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "0101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "0101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "0101040203": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040301": "é€è¾½å®äº¤æ˜“",
    "0101040321": "é€ååŒ—äº¤æ˜“", 
    "0101040322": "é€å±±ä¸œäº¤æ˜“",
    "0101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "0102020101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "0102020301": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "0102010101": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "0102010201": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "0202030001": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "0202030002": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "0101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "0101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "0101100101": "åå·®è€ƒæ ¸è´¹ç”¨",
    "0101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101100001": "æ—¥èåˆäº¤æ˜“",
    "101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101100001": "æ—¥èåˆäº¤æ˜“",
    "101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "101100101": "åå·®è€ƒæ ¸è´¹ç”¨"
}
TRADE_KEYWORDS = {
    "ä¼˜å…ˆå‘ç”µ": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "ä»£ç†è´­ç”µ": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“",
    "ç›´æ¥äº¤æ˜“": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "ç»¿è‰²ç”µåŠ›": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æ±Ÿè‹": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æµ™æ±Ÿ": "é€æµ™æ±Ÿäº¤æ˜“",
    "é€æµ™æ±Ÿçœé—´": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€ä¸Šæµ·": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€è¾½å®": "é€è¾½å®äº¤æ˜“",
    "é€ååŒ—": "é€ååŒ—äº¤æ˜“",
    "é€å±±ä¸œ": "é€å±±ä¸œäº¤æ˜“",
    "æ—¥èåˆ": "æ—¥èåˆäº¤æ˜“",
    "ç°è´§æ—¥å‰": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "ç°è´§å®æ—¶": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "é˜»å¡è´¹ç”¨": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "ä»·å·®è´¹ç”¨": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "ç°è´§ç»“ç®—": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "è¾…åŠ©æœåŠ¡": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "åå·®è€ƒæ ¸": "åå·®è€ƒæ ¸è´¹ç”¨"
}
DATA_RULES = {
    "ç”µé‡(å…†ç“¦æ—¶)": {"min": -1000, "max": 5000},
    "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": {"min": 0, "max": 2000},
    "ç”µè´¹(å…ƒ)": {"min": -10000, "max": 10000000}
}
# æ˜ç¡®æ’é™¤æ— å…³å…³é”®è¯ï¼ˆé¿å…çº³å…¥åœºç«™åç§°ï¼‰
EXCLUDE_KEYWORDS = ["è®¡é‡ç”µé‡", "ç”µé‡", "ç”µä»·", "ç”µè´¹", "åˆè®¡", "å°è®¡", "ç¼–åˆ¶", "å®¡æ ¸"]
STATION_SPLIT_KEYWORDS = ["æœºç»„", "æœºç»„åç§°", "åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", "Aé£ç”µåœº", "Bé£ç”µåœº"]
STATION_CORE_NAMES = ["åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", "æ™¶ç››å…‰ä¼ç”µç«™"]  # ä»…ä¿ç•™çº¯å‡€æ ¸å¿ƒåç§°
STATION_TYPE_KEYWORDS = ["é£ç”µåœº", "å…‰ä¼ç”µç«™", "å‚¨èƒ½ç”µç«™", "ç”µç«™", "åœºç«™"]

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆç²¾å‡†æˆªæ–­ä¿®å¤ï¼‰ ----------------------
def remove_redundant_text(text):
    if not text:
        return ""
    cleaned = str(text).strip()
    # ä¿ç•™â€œæœºç»„â€å…³é”®è¯ï¼Œæ¸…ç†å…¶ä»–å†—ä½™
    for keyword in REDUNDANT_KEYWORDS:
        if keyword != "æœºç»„":
            cleaned = cleaned.replace(keyword, "")
    # æ¸…ç†å•ä¸ªå­—ç¬¦å’Œä¹±ç 
    single_watermarks = ["ä¾", "å…°", "å", "åˆ", "å¿", "ç”µ", "åŠ›", "å‘", "é™"]
    for char in single_watermarks:
        cleaned = cleaned.replace(char, "")
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.\-\: ]', '', cleaned)
    return cleaned.strip()

def truncate_station_name(station_name):
    """æ–°å¢ï¼šç²¾å‡†æˆªæ–­åœºç«™åç§°ï¼Œæ’é™¤æ— å…³æ•°æ®ï¼ˆæ ¸å¿ƒä¿®å¤ï¼‰"""
    if not station_name:
        return "æœªçŸ¥åœºç«™"
    # 1. ä¼˜å…ˆåŒ¹é…æ ¸å¿ƒåç§°ï¼Œç›´æ¥è¿”å›ï¼ˆæœ€ç²¾å‡†ï¼‰
    for core_name in STATION_CORE_NAMES:
        if core_name in station_name:
            return core_name
    # 2. é‡åˆ°æ’é™¤å…³é”®è¯åˆ™æˆªæ–­ï¼ˆå¦‚â€œåŒå‘Aé£ç”µåœº è®¡é‡ç”µé‡â€â†’â€œåŒå‘Aé£ç”µåœºâ€ï¼‰
    for exclude_key in EXCLUDE_KEYWORDS:
        if exclude_key in station_name:
            return station_name.split(exclude_key)[0].strip()
    # 3. æŒ‰æ ‡ç‚¹ç¬¦å·æˆªæ–­
    for separator in ["ï¼Œ", "ã€‚", "ï¼›", "ï¼š", "\n", "\t"]:
        if separator in station_name:
            return station_name.split(separator)[0].strip()
    # 4. ä¿ç•™å«åœºç«™ç±»å‹çš„éƒ¨åˆ†ï¼ˆé¿å…è¿‡é•¿ï¼‰
    for type_key in STATION_TYPE_KEYWORDS:
        if type_key in station_name:
            type_idx = station_name.find(type_key)
            return station_name[:type_idx + len(type_key)].strip()
    return station_name.strip()

def extract_station_from_text(pdf_text):
    """ä¿®å¤ï¼šæ­£åˆ™å¢åŠ è¾¹ç•Œé™åˆ¶ï¼Œé¿å…åŒ…å«æ— å…³æ•°æ®"""
    clean_text = remove_redundant_text(pdf_text)
    # æ­£åˆ™ä¼˜åŒ–ï¼šåŒ¹é…â€œæœºç»„ï¼šXXXé£ç”µåœºâ€ä¸”æ’é™¤åç»­æ— å…³å†…å®¹
    station_patterns = [
        # åŒ¹é…â€œæœºç»„ï¼šåŒå‘Aé£ç”µåœºâ€ï¼Œä¸”åé¢ä¸åŒ…å«æ’é™¤å…³é”®è¯
        r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\nè®¡é‡ç”µé‡]+é£ç”µåœº)',
        r'æœºç»„åç§°[:ï¼š\s]*([^ï¼Œã€‚\nè®¡é‡ç”µé‡]+é£ç”µåœº)',
        # ç²¾å‡†åŒ¹é…åŒå‘A/Bé£ç”µåœº
        r'(åŒå‘[AB]é£ç”µåœº)',
        r'([^ï¼Œã€‚\n]+åŒå‘[AB]é£ç”µåœº)'
    ]
    for pattern in station_patterns:
        match = re.search(pattern, clean_text)
        if match:
            raw_name = match.group(1).strip()
            return truncate_station_name(raw_name)  # æˆªæ–­å¤„ç†
    # å…œåº•ï¼šæå–å«åœºç«™ç±»å‹çš„åç§°
    for type_key in STATION_TYPE_KEYWORDS:
        match = re.search(r'([^ï¼Œã€‚\n]+' + type_key + ')', clean_text)
        if match:
            raw_name = match.group(1).strip()
            return truncate_station_name(raw_name)
    return "æœªçŸ¥åœºç«™"

def extract_station_from_filename(file_name):
    """ä¿®å¤ï¼šä»æ–‡ä»¶åæå–æ—¶ä¹Ÿåšæˆªæ–­"""
    if not file_name:
        return "æœªçŸ¥åœºç«™"
    name_patterns = [
        r'(åŒå‘[AB]é£ç”µåœº)',
        r'([^_]+åŒå‘[AB]é£ç”µåœº[^_]+)',
        r'([^_]+é£ç”µåœº)'
    ]
    for pattern in name_patterns:
        match = re.search(pattern, file_name)
        if match:
            raw_name = match.group(1).strip()
            return truncate_station_name(raw_name)
    return "æœªçŸ¥åœºç«™"

def clean_station_name(station_name):
    """ä¿®å¤ï¼šå¢åŠ æˆªæ–­æ­¥éª¤ï¼Œç¡®ä¿çº¯å‡€"""
    if not station_name or station_name == "æœªçŸ¥åœºç«™":
        return "æœªçŸ¥åœºç«™"
    # 1. å…ˆæ¸…ç†å†—ä½™æ–‡æœ¬
    cleaned = remove_redundant_text(station_name)
    # 2. å…³é”®æ­¥éª¤ï¼šç²¾å‡†æˆªæ–­
    truncated = truncate_station_name(cleaned)
    # 3. æœ€ç»ˆéªŒè¯æ˜¯å¦ä¸ºæ ¸å¿ƒåç§°
    for core_name in STATION_CORE_NAMES:
        if core_name in truncated:
            return core_name
    return truncated if truncated else "æœªçŸ¥åœºç«™"

def safe_convert_to_numeric(value, data_type=""):
    if value is None or pd.isna(value) or value == '':
        return None
    val_str = remove_redundant_text(value)
    if re.match(r'^\d{9,10}$', val_str) or val_str in ['-', '.', '', 'â€”', 'â€”â€”']:
        return None
    try:
        cleaned = re.sub(r'[^\d\-\.]', '', val_str.replace('ï¼Œ', ',').replace('ã€‚', '.'))
        if not cleaned or cleaned in ['-', '.', '-.' , '-.']:
            return None
        num = float(cleaned)
        if data_type in DATA_RULES:
            rule = DATA_RULES[data_type]
            if num < rule["min"] or num > rule["max"]:
                return None
        return num
    except (ValueError, TypeError):
        return None

def extract_company_info(pdf_text, file_name):
    clean_text = remove_redundant_text(pdf_text)
    company_name = "æœªçŸ¥å‘ç”µå…¬å¸"
    company_match = re.search(r'å…¬å¸åç§°[:ï¼š]\s*([^ï¼Œã€‚\n]+å…¬å¸)', clean_text)
    if company_match:
        company_name = company_match.group(1).strip()
    else:
        company_match = re.search(r'([^_]+å…¬å¸|[^_]+å‘ç”µ)', file_name)
        if company_match:
            company_name = company_match.group(1).strip()
    return company_name

def extract_clear_date(pdf_text, file_name):
    clean_text = remove_redundant_text(pdf_text)
    date = None
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'ç»“ç®—æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'(\d{4}-\d{1,2}-\d{1,2})'
    ]
    for pattern in date_patterns:
        match = re.search(pattern, clean_text)
        if match:
            date_str = match.group(1).strip()
            if "å¹´" in date_str:
                date_str = date_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            date = date_str
            break
    if not date:
        date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2}|\d{8})', file_name)
        if date_match:
            date_str = date_match.group(1)
            if len(date_str) == 8:
                date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date = date_str
    return date

def split_double_station_tables(all_tables, pdf_text, file_name):
    """ä¿®å¤ï¼šæ‹†åˆ†æ—¶å¯¹åœºç«™åç§°åšæˆªæ–­å¤„ç†"""
    clean_text = remove_redundant_text(pdf_text)
    merged_rows = []
    for table in all_tables:
        if not table:
            continue
        for row in table:
            cleaned_row = [remove_redundant_text(cell) for cell in row]
            if any(cell.strip() != "" for cell in cleaned_row):
                merged_rows.append(cleaned_row)
    
    if not merged_rows:
        text_station = extract_station_from_text(pdf_text)
        if text_station != "æœªçŸ¥åœºç«™":
            return [(text_station, [])]
        file_station = extract_station_from_filename(file_name)
        return [(file_station, [])]
    
    station_segments = []
    current_segment = []
    # åˆå§‹åœºç«™ä»æ–‡æœ¬æå–ï¼ˆå·²æˆªæ–­ï¼‰
    current_station = extract_station_from_text(pdf_text)
    
    for row in merged_rows:
        row_str = ''.join(row).replace(" ", "")
        has_station_key = any(keyword in row_str for keyword in STATION_SPLIT_KEYWORDS)
        
        if has_station_key:
            # ä¿å­˜ä¸Šä¸€æ®µï¼šåœºç«™åç§°å·²æˆªæ–­
            if current_segment:
                cleaned_station = clean_station_name(current_station)
                station_segments.append((cleaned_station, current_segment))
            # æå–å½“å‰åœºç«™å¹¶æˆªæ–­
            row_text = ' '.join([remove_redundant_text(cell) for cell in row])
            station_match = re.search(r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\n]+)', row_text) or re.search(r'(åŒå‘[AB]é£ç”µåœº)', row_text)
            if station_match:
                current_station = truncate_station_name(station_match.group(1))
            current_segment = [row]
        else:
            current_segment.append(row)
    
    # ä¿å­˜æœ€åä¸€æ®µ
    if current_segment:
        cleaned_station = clean_station_name(current_station)
        if cleaned_station == "æœªçŸ¥åœºç«™":
            cleaned_station = extract_station_from_filename(file_name)
        station_segments.append((cleaned_station, current_segment))
    
    # è¿‡æ»¤æ— æ•ˆæ®µ
    valid_segments = [(station, seg) for station, seg in station_segments if len(seg) >= 2 or station != "æœªçŸ¥åœºç«™"]
    if not valid_segments:
        fallback_station = extract_station_from_filename(file_name)
        valid_segments = [(fallback_station, merged_rows)]
    
    return valid_segments

def get_trade_name(trade_code, trade_text):
    if trade_code in TRADE_CODE_MAP:
        return TRADE_CODE_MAP[trade_code]
    for key, name in TRADE_KEYWORDS.items():
        if key in trade_text:
            return name
    return "æœªè¯†åˆ«ç§‘ç›®"

def parse_single_station_data(station_name, table_segment, company_name, clear_date):
    trade_records = []
    valid_rows = []
    
    for row in table_segment:
        if not row:
            continue
        row_clean = [remove_redundant_text(cell) for cell in row]
        row_str = ''.join(row_clean).replace(" ", "")
        is_empty = all(cell == '' for cell in row_clean)
        is_header = any(keyword in row_str for keyword in ["ç§‘ç›®ç¼–ç ", "ç»“ç®—ç±»å‹", "ç”µé‡", "ç”µä»·", "ç”µè´¹"])
        
        has_code = any(re.match(r'^\d{9,10}$', cell.replace(" ", "")) for cell in row_clean)
        has_trade_key = any(key in row_str for key in TRADE_KEYWORDS.keys())
        has_valid_data = any(safe_convert_to_numeric(cell) is not None for cell in row_clean if cell not in ['', '-'])
        
        if (has_code or has_trade_key or has_valid_data) and not is_empty and not is_header:
            valid_rows.append(row_clean)
    
    if len(valid_rows) < 2:
        return trade_records
    
    cols = {"code": -1, "name": -1, "qty": -1, "price": -1, "fee": -1}
    header_idx = -1
    for idx, row in enumerate(valid_rows[:3]):
        row_str = ''.join(row).replace(" ", "")
        if "ç»“ç®—ç±»å‹" in row_str:
            header_idx = idx
            break
    if header_idx == -1:
        header_idx = 0
    header_row = valid_rows[header_idx]
    
    for col_idx, cell in enumerate(header_row):
        cell_clean = remove_redundant_text(cell).lower()
        if "ç¼–ç " in cell_clean:
            cols["code"] = col_idx
        elif "ç»“ç®—ç±»å‹" in cell_clean:
            cols["name"] = col_idx
        elif "ç”µé‡" in cell_clean and "ä»·" not in cell_clean:
            cols["qty"] = col_idx
        elif "ç”µä»·" in cell_clean or "å•ä»·" in cell_clean:
            cols["price"] = col_idx
        elif "ç”µè´¹" in cell_clean or "é‡‘é¢" in cell_clean:
            cols["fee"] = col_idx
    
    if any(v == -1 for v in cols.values()) and len(header_row) >= 5:
        cols = {"code": 0, "name": 1, "qty": 2, "price": 3, "fee": 4}
    
    data_start_idx = header_idx + 1
    for row_idx in range(data_start_idx, len(valid_rows)):
        row = valid_rows[row_idx]
        row_str = ''.join(row).replace(" ", "")
        is_subtotal = "å°è®¡" in row_str
        
        if "åˆè®¡" in row_str and not is_subtotal:
            continue
        
        trade_code = row[cols["code"]].strip().replace(" ", "") if (cols["code"] < len(row)) else ""
        trade_text = row[cols["name"]].strip() if (cols["name"] < len(row)) else ""
        trade_name = get_trade_name(trade_code, trade_text)
        
        if trade_name == "æœªè¯†åˆ«ç§‘ç›®" and not is_subtotal:
            continue
        
        if is_subtotal:
            subtotal_qty = None
            subtotal_fee = None
            nums = [safe_convert_to_numeric(cell, "ç”µé‡(å…†ç“¦æ—¶)") for cell in row if safe_convert_to_numeric(cell) is not None]
            fee_nums = [safe_convert_to_numeric(cell, "ç”µè´¹(å…ƒ)") for cell in row if safe_convert_to_numeric(cell) is not None]
            if nums:
                subtotal_qty = nums[0]
            if fee_nums:
                subtotal_fee = fee_nums[-1]
            trade_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "å½“æ—¥å°è®¡",
                "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
                "ç”µé‡(å…†ç“¦æ—¶)": subtotal_qty,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": subtotal_fee
            })
            continue
        
        quantity = safe_convert_to_numeric(row[cols["qty"]], "ç”µé‡(å…†ç“¦æ—¶)") if (cols["qty"] < len(row)) else None
        price = safe_convert_to_numeric(row[cols["price"]], "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)") if (cols["price"] < len(row)) else None
        fee = safe_convert_to_numeric(row[cols["fee"]], "ç”µè´¹(å…ƒ)") if (cols["fee"] < len(row)) else None
        
        if "é˜»å¡è´¹ç”¨" in trade_name or "ä»·å·®è´¹ç”¨" in trade_name or "è¾…åŠ©æœåŠ¡" in trade_name or "åå·®è€ƒæ ¸" in trade_name:
            quantity = None
            price = None
        
        if quantity is None and fee is None:
            continue
        
        trade_records.append({
            "å…¬å¸åç§°": company_name,
            "åœºç«™åç§°": station_name,
            "æ¸…åˆ†æ—¥æœŸ": clear_date,
            "ç§‘ç›®åç§°": trade_name,
            "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
            "ç”µé‡(å…†ç“¦æ—¶)": quantity,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": price,
            "ç”µè´¹(å…ƒ)": fee
        })
    
    return trade_records

# ---------------------- ä¸»è§£æå‡½æ•° ----------------------
def parse_pdf_final(file_obj, file_name):
    try:
        file_obj.seek(0)
        file_bytes = BytesIO(file_obj.read())
        file_bytes.seek(0)
        
        all_text = ""
        all_tables = []
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                all_text += text + "\n"
                tables = page.extract_tables({
                    "vertical_strategy": "lines_strict",
                    "horizontal_strategy": "lines_strict",
                    "snap_tolerance": 1,
                    "join_tolerance": 1,
                    "edge_min_length": 3
                })
                all_tables.extend(tables)
        
        company_name = extract_company_info(all_text, file_name)
        clear_date = extract_clear_date(all_text, file_name)
        station_segments = split_double_station_tables(all_tables, all_text, file_name)
        
        all_records = []
        for station_name, table_segment in station_segments:
            # æœ€ç»ˆç¡®ä¿åœºç«™åç§°å·²æˆªæ–­
            final_station = clean_station_name(station_name)
            station_data = parse_single_station_data(final_station, table_segment, company_name, clear_date)
            all_records.extend(station_data)
        
        # å»é‡
        unique_records = []
        seen_keys = set()
        for rec in all_records:
            key = f"{rec['åœºç«™åç§°']}_{rec['ç§‘ç›®åç§°']}_{rec['åŸå§‹ç§‘ç›®ç¼–ç ']}"
            if key not in seen_keys:
                seen_keys.add(key)
                unique_records.append(rec)
        
        if not unique_records:
            fallback_station = extract_station_from_filename(file_name)
            unique_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": fallback_station,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "æ— æœ‰æ•ˆæ•°æ®",
                "åŸå§‹ç§‘ç›®ç¼–ç ": "",
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
                "ç”µé‡(å…†ç“¦æ—¶)": None,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": None
            })
        
        return unique_records
    
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {str(e)}")
        fallback_station = extract_station_from_filename(file_name)
        return [{
            "å…¬å¸åç§°": "æœªçŸ¥å‘ç”µå…¬å¸",
            "åœºç«™åç§°": fallback_station,
            "æ¸…åˆ†æ—¥æœŸ": None,
            "ç§‘ç›®åç§°": "è§£æå¤±è´¥",
            "åŸå§‹ç§‘ç›®ç¼–ç ": "",
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
            "ç”µé‡(å…†ç“¦æ—¶)": None,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
            "ç”µè´¹(å…ƒ)": None
        }]

# ---------------------- Streamlitåº”ç”¨ ----------------------
def main():
    st.set_page_config(page_title="é€šç”¨æ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·ï¼ˆåœºç«™ç²¾å‡†ç‰ˆï¼‰", layout="wide")
    
    st.title("ğŸ“Š é€šç”¨ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆåŒåœºç«™ç²¾å‡†ç‰ˆï¼‰")
    st.markdown("**æ ¸å¿ƒä¿®å¤ï¼šåœºç«™åç§°ç²¾å‡†æˆªæ–­ | æ’é™¤è®¡é‡ç”µé‡ç­‰æ— å…³æ•°æ® | åŒå‘A/Bçº¯å‡€æ˜¾ç¤º**")
    st.divider()
    
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆæ”¯æŒåŒåœºç«™/å¤šé¡µé¢ï¼‰",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸš€ å¼€å§‹æ‰¹é‡æå–", type="primary"):
        st.divider()
        st.subheader("âš™ï¸ å¤„ç†è¿›åº¦")
        
        all_results = []
        progress_bar = st.progress(0)
        
        for idx, file in enumerate(uploaded_files):
            st.write(f"æ­£åœ¨å¤„ç†ï¼š{file.name}")
            file_results = parse_pdf_final(file, file.name)
            all_results.extend(file_results)
            progress_bar.progress((idx + 1) / len(uploaded_files))
            file.close()
        
        progress_bar.empty()
        
        df = pd.DataFrame(all_results).fillna("")
        display_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "åŸå§‹ç§‘ç›®ç¼–ç ", "åŸå§‹ç§‘ç›®æ–‡æœ¬", "ç”µé‡(å…†ç“¦æ—¶)", 
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_display = df[[col for col in display_cols if col in df.columns]]
        
        st.subheader("ğŸ“ˆ æ‰¹é‡æå–ç»“æœï¼ˆåœºç«™åç§°çº¯å‡€ï¼‰")
        def highlight_rows(row):
            if row["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                return ["background-color: #e6f3ff"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Aé£ç”µåœº":
                return ["background-color: #f0fff4"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Bé£ç”µåœº":
                return ["background-color: #fff8f0"] * len(row)
            else:
                return [""] * len(row)
        styled_df = df_display.style.apply(highlight_rows, axis=1)
        st.dataframe(styled_df, use_container_width=True)
        
        total_stations = df["åœºç«™åç§°"].nunique()
        total_trades = len(df[(df["ç§‘ç›®åç§°"] != "å½“æ—¥å°è®¡") & (df["ç§‘ç›®åç§°"] != "æ— æœ‰æ•ˆæ•°æ®") & (df["ç§‘ç›®åç§°"] != "è§£æå¤±è´¥")])
        subtotal_count = len(df[df["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡"])
        st.info(f"**ç»Ÿè®¡ï¼š** è¦†ç›–åœºç«™ {total_stations} ä¸ª | æœ‰æ•ˆç§‘ç›® {total_trades} ä¸ª | å°è®¡è¡Œ {subtotal_count} ä¸ª")
        
        # ä¸‹è½½Excel
        download_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "ç”µé‡(å…†ç“¦æ—¶)", "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_download = df[[col for col in download_cols if col in df.columns]]
        
        output = BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_download.to_excel(writer, index=False, sheet_name="å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®")
            ws = writer.sheets["å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®"]
            light_blue = PatternFill(start_color="E6F3FF", end_color="E6F3FF", fill_type="solid")
            for row in range(2, len(df_download) + 2):
                if df_download.iloc[row-2]["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                    for col in range(1, len(df_download.columns) + 1):
                        ws.cell(row=row, column=col).fill = light_blue
        
        output.seek(0)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excelï¼ˆä¸å«åŸå§‹ç¼–ç /æ–‡æœ¬ï¼‰",
            data=output,
            file_name=f"å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        st.success("âœ… æå–å®Œæˆï¼åœºç«™åç§°å·²ç²¾å‡†æˆªæ–­ï¼Œä»…ä¿ç•™â€œåŒå‘Aé£ç”µåœºâ€â€œåŒå‘Bé£ç”µåœºâ€ï¼Œæ— æ— å…³æ•°æ®")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ åŒåœºç«™ï¼ˆå¦‚åŒå‘A/Bé£ç”µåœºï¼‰çš„ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•PDF")

if __name__ == "__main__":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    main()
