import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO
import sys
import os
# æ–°å¢ï¼šå¯¼å…¥openpyxlæ ·å¼æ¨¡å—ï¼ˆä¿®å¤Excelæ ·å¼é”™è¯¯ï¼‰
from openpyxl.styles import PatternFill

# å¿½ç•¥æ ·å¼è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆè¡¥å……æ›´å¤šåŒ¹é…é¡¹ï¼‰ ----------------------
WATERMARK_KEYWORDS = ["ååˆèƒ½æº", "å¤§åº†æ™¶ç››", "å¤ªé˜³èƒ½å‘ç”µ", "å†…éƒ¨ä½¿ç”¨", "CONFIDENTIAL", "è‰ç¨¿", "ååˆ"]
TRADE_CODE_MAP = {
    "0101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "0101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "0101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "0101040203": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040301": "é€è¾½å®äº¤æ˜“",
    "0101040321": "é€ååŒ—äº¤æ˜“", 
    "0101040322": "é€å±±ä¸œäº¤æ˜“",
    "0101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "0102020101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "0102020301": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "0102010101": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "0102010201": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "0202030001": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "0202030002": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "0101050101": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101060101": "æ—¥èåˆäº¤æ˜“",
    "0101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "0101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "0101100101": "åå·®è€ƒæ ¸è´¹ç”¨"
}
TRADE_NAME_KEYWORDS = {
    "ä¼˜å…ˆå‘ç”µ": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "ä»£ç†è´­ç”µ": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“",
    "ç›´æ¥äº¤æ˜“": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "é€ä¸Šæµ·": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€è¾½å®": "é€è¾½å®äº¤æ˜“",
    "é€ååŒ—": "é€ååŒ—äº¤æ˜“",
    "é€å±±ä¸œ": "é€å±±ä¸œäº¤æ˜“",
    "é€æµ™æ±Ÿ": "é€æµ™æ±Ÿäº¤æ˜“",
    "ç°è´§æ—¥å‰": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "ç°è´§å®æ—¶": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "é˜»å¡è´¹ç”¨": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "ä»·å·®è´¹ç”¨": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "è¾…åŠ©æœåŠ¡": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "åå·®è€ƒæ ¸": "åå·®è€ƒæ ¸è´¹ç”¨"
}
SPECIAL_TRADES = ["ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨", "çœé—´çœå†…ä»·å·®è´¹ç”¨", "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š", "åå·®è€ƒæ ¸è´¹ç”¨"]

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆå…¨ä¿®å¤ï¼‰ ----------------------
def remove_watermark(text):
    if not text:
        return ""
    cleaned_text = text
    for keyword in WATERMARK_KEYWORDS:
        cleaned_text = cleaned_text.replace(keyword, "")
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    cleaned_text = re.sub(r'[\x00-\x1F\x7F]', '', cleaned_text)
    return cleaned_text.strip()

def safe_convert_to_numeric(value):
    if value is None or pd.isna(value) or value == '':
        return None
    val_str = remove_watermark(str(value)).strip()
    if re.match(r'^\d{9,10}$', val_str) or val_str in ['-', '.', '', 'â€”', 'â€”â€”']:
        return None
    try:
        cleaned = re.sub(r'[^\d.-]', '', val_str.replace('ï¼Œ', ',').replace('ã€‚', '.'))
        if not cleaned or cleaned in ['-', '.']:
            return None
        num = float(cleaned)
        return num
    except (ValueError, TypeError):
        return None

def extract_clear_date(pdf_text):
    # è¡¥å……æ›´å¤šæ—¥æœŸæ ¼å¼
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}[æ—¥]?)',
        r'ç»“ç®—æ—¥æœŸ[:ï¼š]\s*(\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}[æ—¥]?)',
        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)\s*æ¸…åˆ†',
        r'(\d{4}-\d{1,2}-\d{1,2})\s*ç°è´§æ—¥æ¸…åˆ†',
        r'æ—¥æœŸ[:ï¼š]\s*(\d{4}[å¹´/-]\d{1,2}[æœˆ/-]\d{1,2}[æ—¥]?)',  # è¡¥å……â€œæ—¥æœŸï¼šâ€æ ¼å¼
        r'(\d{4}\.\d{1,2}\.\d{1,2})'  # è¡¥å……â€œ2026.01.01â€æ ¼å¼
    ]
    for pattern in date_patterns:
        match = re.search(pattern, pdf_text)
        if match:
            date_str = match.group(1)
            date_str = re.sub(r'[å¹´æœˆæ—¥]', '-', date_str).rstrip('-')
            parts = date_str.split('-')
            if len(parts) == 3:
                year, month, day = parts
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            elif len(date_str.split('.')) == 3:
                year, month, day = date_str.split('.')
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
    return None

def extract_base_info(pdf_text):
    clean_text = remove_watermark(pdf_text)
    lines = clean_text.split('\n')
    
    # ä¿®å¤1ï¼šæ”¾å®½å…¬å¸/åœºç«™åç§°åŒ¹é…ï¼ˆä»æ–‡æœ¬ä¸­æ‰¾â€œåœºç«™â€â€œç”µç«™â€å…³é”®è¯ï¼‰
    station_name = "æœªçŸ¥åœºç«™"
    company_name = "æœªçŸ¥å…¬å¸"
    for line in lines:
        if "åœºç«™" in line or "ç”µç«™" in line:
            match = re.search(r'([^ï¼Œã€‚\n]+[åœºç«™|ç”µç«™])', line)
            if match:
                station_name = match.group(1).strip()
        if "å…¬å¸" in line:
            match = re.search(r'([^ï¼Œã€‚\n]+å…¬å¸)', line)
            if match:
                company_name = match.group(1).strip()
    
    # ä¿®å¤2ï¼šæå–æ—¥æœŸ
    date = extract_clear_date(clean_text)
    
    # ä¿®å¤3ï¼šæå–å°è®¡/åˆè®¡
    subtotal_quantity = None
    subtotal_fee = None
    for line in lines:
        line_clean = remove_watermark(line).replace(' ', '').replace(',', '')
        if 'å°è®¡' in line_clean:
            qty_match = re.search(r'å°è®¡ç”µé‡[:ï¼š]([\d\.]+)|ç”µé‡[:ï¼š]([\d\.]+)', line_clean)
            fee_match = re.search(r'å°è®¡ç”µè´¹[:ï¼š]([\d\.]+)|ç”µè´¹[:ï¼š]([\d\.]+)', line_clean)
            if qty_match:
                subtotal_quantity = safe_convert_to_numeric(next(g for g in qty_match.groups() if g))
            if fee_match:
                subtotal_fee = safe_convert_to_numeric(next(g for g in fee_match.groups() if g))
    
    return station_name, company_name, date, subtotal_quantity, subtotal_fee

def locate_table_columns(table_rows):
    # ä¿®å¤4ï¼šè¡¥å……æ›´å¤šè¡¨å¤´å…³é”®è¯ï¼ˆåŒ¹é…PDFå®é™…è¡¨å¤´ï¼‰
    target_columns = {
        "ç§‘ç›®ç¼–ç ": ["ç§‘ç›®ç¼–ç ", "ç¼–ç "],
        "ç§‘ç›®åç§°": ["ç§‘ç›®åç§°", "ç»“ç®—ç±»å‹", "åç§°"],
        "äº¤æ˜“ç”µé‡": ["äº¤æ˜“ç”µé‡", "ç”µé‡", "å…†ç“¦æ—¶", "MWh"],
        "ç»“ç®—ç”µä»·": ["ç»“ç®—ç”µä»·", "ç”µä»·", "å…ƒ/å…†ç“¦æ—¶", "å…ƒ/MWh"],
        "ç»“ç®—ç”µè´¹": ["ç»“ç®—ç”µè´¹", "ç”µè´¹", "é‡‘é¢", "å…ƒ"]
    }
    final_cols = {k: -1 for k in target_columns.keys()}
    used_cols = set()
    
    for row_idx, row in enumerate(table_rows[:3]):
        for col_idx, cell in enumerate(row):
            cell_clean = remove_watermark(str(cell)).lower().strip()
            for col_name, keywords in target_columns.items():
                if any(key in cell_clean for key in keywords) and col_idx not in used_cols:
                    final_cols[col_name] = col_idx
                    used_cols.add(col_idx)
                    break
    return final_cols

def correct_trade_name(trade_name):
    if not trade_name:
        return "æœªçŸ¥ç§‘ç›®"
    clean_name = remove_watermark(trade_name).strip()
    for keyword, correct_name in TRADE_NAME_KEYWORDS.items():
        if keyword in clean_name:
            return correct_name
    return clean_name if clean_name else "æœªçŸ¥ç§‘ç›®"

def extract_trade_data_from_tables(tables, station_name, clear_date):
    trade_records = []
    for table in tables:
        if len(table) < 3:
            continue
        clean_table = []
        for row in table:
            clean_row = [remove_watermark(str(cell)) for cell in row]
            if any(cell.strip() != '' for cell in clean_row):
                clean_table.append(clean_row)
        if len(clean_table) < 3:
            continue
        
        final_cols = locate_table_columns(clean_table)
        code_col = final_cols["ç§‘ç›®ç¼–ç "]
        name_col = final_cols["ç§‘ç›®åç§°"]
        qty_col = final_cols["äº¤æ˜“ç”µé‡"]
        price_col = final_cols["ç»“ç®—ç”µä»·"]
        fee_col = final_cols["ç»“ç®—ç”µè´¹"]
        
        if (code_col == -1 and name_col == -1) or (qty_col == -1 and fee_col == -1):
            continue
        
        for row_idx, row in enumerate(clean_table):
            if row_idx < 1:  # ä»…è·³è¿‡ç¬¬1è¡Œè¡¨å¤´
                continue
            row_clean = [cell.strip() for cell in row]
            if 'åˆè®¡' in ''.join(row_clean) and 'å°è®¡' not in ''.join(row_clean):
                continue
            
            trade_code = row[code_col].strip() if (code_col != -1 and code_col < len(row)) else ''
            raw_name = row[name_col].strip() if (name_col != -1 and name_col < len(row)) else ''
            trade_name = TRADE_CODE_MAP.get(trade_code, correct_trade_name(raw_name))
            
            # ä¿®å¤5ï¼šç¡®ä¿åˆ—ç´¢å¼•ä¸è¶Šç•Œ
            quantity = safe_convert_to_numeric(row[qty_col]) if (qty_col != -1 and qty_col < len(row)) else None
            price = safe_convert_to_numeric(row[price_col]) if (price_col != -1 and price_col < len(row)) else None
            fee = safe_convert_to_numeric(row[fee_col]) if (fee_col != -1 and fee_col < len(row)) else None
            
            if trade_name in SPECIAL_TRADES:
                quantity = None
                price = None
            
            is_subtotal = 'å°è®¡' in ''.join(row_clean)
            if not is_subtotal and (quantity is None and fee is None):
                continue
            
            trade_records.append({
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": trade_name,
                "æ˜¯å¦å°è®¡è¡Œ": is_subtotal,
                "ç”µé‡(å…†ç“¦æ—¶)": quantity if not is_subtotal else quantity,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": price if not is_subtotal else None,
                "ç”µè´¹(å…ƒ)": fee
            })
    return trade_records

def parse_pdf_file(file_obj, file_name):
    try:
        file_obj.seek(0)
        file_bytes = BytesIO(file_obj.read())
        file_bytes.seek(0)
        
        all_text = ""
        all_tables = []
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                all_text += remove_watermark(text) + "\n"
                tables = page.extract_tables({
                    "vertical_strategy": "lines",
                    "horizontal_strategy": "lines",
                    "snap_tolerance": 2,
                    "join_tolerance": 2
                })
                all_tables.extend(tables)
        
        station_name, company_name, clear_date, subtotal_qty, subtotal_fee = extract_base_info(all_text)
        trade_records = extract_trade_data_from_tables(all_tables, station_name, clear_date)
        
        # è¡¥å……å°è®¡è¡Œ
        if subtotal_qty or subtotal_fee:
            trade_records.append({
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "å½“æ—¥å°è®¡",
                "æ˜¯å¦å°è®¡è¡Œ": True,
                "ç”µé‡(å…†ç“¦æ—¶)": subtotal_qty,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": subtotal_fee
            })
        
        return trade_records
    except Exception as e:
        st.error(f"PDFè§£æå¤±è´¥ï¼ˆ{file_name}ï¼‰: {str(e)}")
        return [{
            "åœºç«™åç§°": "æœªçŸ¥åœºç«™",
            "æ¸…åˆ†æ—¥æœŸ": None,
            "ç§‘ç›®åç§°": "è§£æå¤±è´¥",
            "æ˜¯å¦å°è®¡è¡Œ": False,
            "ç”µé‡(å…†ç“¦æ—¶)": None,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
            "ç”µè´¹(å…ƒ)": None
        }]

# ---------------------- Streamlit åº”ç”¨ï¼ˆä¿®å¤Excelæ ·å¼é”™è¯¯ï¼‰ ----------------------
def main():
    st.set_page_config(page_title="æ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·ï¼ˆä¿®å¤ç‰ˆï¼‰", layout="wide")
    
    st.title("ğŸ“Š æ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆç¨³å®šç‰ˆï¼‰")
    st.markdown("**å·²ä¿®å¤ï¼šExcelæ ·å¼é”™è¯¯ | æ—¥æœŸ/åœºç«™æå– | æ•°æ®æ˜ å°„åå·®**")
    st.divider()
    
    uploaded_files = st.file_uploader(
        "æ”¯æŒPDFæ ¼å¼ï¼ˆå•æ–‡ä»¶ä¸Šä¼ ï¼‰",
        type=['pdf'],
        accept_multiple_files=False
    )
    
    if uploaded_files and st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary"):
        st.divider()
        file = uploaded_files
        st.write(f"æ­£åœ¨å¤„ç†ï¼š{file.name}")
        trade_records = parse_pdf_file(file, file.name)
        file.close()
        
        result_df = pd.DataFrame(trade_records)
        col_order = ["åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", "æ˜¯å¦å°è®¡è¡Œ", "ç”µé‡(å…†ç“¦æ—¶)", "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"]
        result_df = result_df[col_order].fillna("")  # ç©ºå€¼æ˜¾ç¤ºä¸ºç©ºå­—ç¬¦ä¸²
        
        # æ˜¾ç¤ºç»“æœ
        st.subheader("ğŸ“ˆ æå–ç»“æœ")
        styled_df = result_df.style.apply(
            lambda row: ['background-color: #f0f8ff' if row["æ˜¯å¦å°è®¡è¡Œ"] else '' for _ in row],
            axis=1
        )
        st.dataframe(styled_df, use_container_width=True)
        
        # ç»Ÿè®¡
        subtotal_count = len(result_df[result_df["æ˜¯å¦å°è®¡è¡Œ"]])
        trade_count = len(result_df[~result_df["æ˜¯å¦å°è®¡è¡Œ"]])
        st.info(f"**ç»Ÿè®¡ï¼š** {trade_count}ä¸ªç§‘ç›® + {subtotal_count}ä¸ªå°è®¡è¡Œ | åœºç«™ï¼š{result_df['åœºç«™åç§°'].iloc[0]} | æ—¥æœŸï¼š{result_df['æ¸…åˆ†æ—¥æœŸ'].iloc[0] or 'å¾…è¯†åˆ«'}")
        
        # ä¿®å¤6ï¼šæ­£ç¡®è®¾ç½®Excelæ ·å¼ï¼ˆç”¨openpyxlï¼‰
        output = BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            result_df.to_excel(writer, index=False, sheet_name="æ•°æ®")
            worksheet = writer.sheets["æ•°æ®"]
            # å®šä¹‰æµ…è“è‰²å¡«å……
            light_blue_fill = PatternFill(start_color="F0F8FF", end_color="F0F8FF", fill_type="solid")
            # éå†è¡Œè®¾ç½®æ ·å¼
            for row_idx in range(2, len(result_df) + 2):
                if result_df.iloc[row_idx - 2]["æ˜¯å¦å°è®¡è¡Œ"]:
                    for col_idx in range(1, len(col_order) + 1):
                        worksheet.cell(row=row_idx, column=col_idx).fill = light_blue_fill
        
        output.seek(0)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excel",
            data=output,
            file_name=f"æ—¥æ¸…åˆ†æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        st.success("âœ… å¤„ç†å®Œæˆï¼")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ PDFæ–‡ä»¶å¼€å§‹å¤„ç†")

if __name__ == "__main__":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    main()
