import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO
import sys
import os
from openpyxl.styles import PatternFill

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆæ¢å¤æ ¸å¿ƒé€»è¾‘ï¼‰ ----------------------
REDUNDANT_KEYWORDS = [
    "å†…éƒ¨ä½¿ç”¨", "CONFIDENTIAL", "è‰ç¨¿", "ç°è´§è¯•ç»“ç®—æœŸé—´", "æ—¥æ¸…åˆ†å•æ®",
    "å…¬å¸åç§°", "ç¼–å·ï¼š", "å•ä½ï¼š", "æ¸…åˆ†æ—¥æœŸ", "åˆè®¡ç”µé‡", "åˆè®¡ç”µè´¹",
    "ç”µèƒ½é‡ç”µè´¹", "å®¡æ‰¹ï¼š", "å®¡æ ¸ï¼š", "ç¼–åˆ¶ï¼š", "åŠ ç›–ç”µå­ç­¾ç« ",
    "ylxxhfd", "yxxchfd", "ä¾å…°å¿ååˆé£åŠ›å‘ç”µæœ‰é™å…¬å¸", "ä¾", "ä¾å…°", "ååˆ",
    "å¿", "é£åŠ›å‘ç”µ", "æœ‰é™å…¬å¸", "å¸"
]
# ç§‘ç›®ç¼–ç -åç§°æ˜ å°„ï¼ˆæ¢å¤å®Œæ•´ï¼‰
TRADE_CODE_MAP = {
    "0101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "0101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "0101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "0101040203": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040301": "é€è¾½å®äº¤æ˜“",
    "0101040321": "é€ååŒ—äº¤æ˜“", 
    "0101040322": "é€å±±ä¸œäº¤æ˜“",
    "0101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "0102020101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "0102020301": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "0102010101": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "0102010201": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "0202030001": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "0202030002": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "0101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "0101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "0101100101": "åå·®è€ƒæ ¸è´¹ç”¨",
    "0101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101100001": "æ—¥èåˆäº¤æ˜“",
    "101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101100001": "æ—¥èåˆäº¤æ˜“",
    "101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "101100101": "åå·®è€ƒæ ¸è´¹ç”¨"
}
TRADE_KEYWORDS = {
    "ä¼˜å…ˆå‘ç”µ": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "ä»£ç†è´­ç”µ": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“",
    "ç›´æ¥äº¤æ˜“": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "ç»¿è‰²ç”µåŠ›": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æ±Ÿè‹": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æµ™æ±Ÿ": "é€æµ™æ±Ÿäº¤æ˜“",
    "é€æµ™æ±Ÿçœé—´": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€ä¸Šæµ·": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€è¾½å®": "é€è¾½å®äº¤æ˜“",
    "é€ååŒ—": "é€ååŒ—äº¤æ˜“",
    "é€å±±ä¸œ": "é€å±±ä¸œäº¤æ˜“",
    "æ—¥èåˆ": "æ—¥èåˆäº¤æ˜“",
    "ç°è´§æ—¥å‰": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "ç°è´§å®æ—¶": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "é˜»å¡è´¹ç”¨": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "ä»·å·®è´¹ç”¨": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "ç°è´§ç»“ç®—": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "è¾…åŠ©æœåŠ¡": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "åå·®è€ƒæ ¸": "åå·®è€ƒæ ¸è´¹ç”¨"
}
DATA_RULES = {
    "ç”µé‡(å…†ç“¦æ—¶)": {"min": -1000, "max": 5000},
    "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": {"min": 0, "max": 2000},
    "ç”µè´¹(å…ƒ)": {"min": -10000, "max": 10000000}
}
# åœºç«™é…ç½®ï¼ˆä¿ç•™å˜ä½“ä½†ä¸å½±å“æ ¸å¿ƒæ•°æ®ï¼‰
STATION_CORE_NAMES = ["åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", "æ™¶ç››å…‰ä¼ç”µç«™"]
STATION_SPLIT_KEYWORDS = ["æœºç»„", "æœºç»„åç§°", "åŒå‘A", "åŒå‘B", "é£åœº", "é£ç”µåœº"]
STATION_TYPE_KEYWORDS = ["é£ç”µåœº", "é£åœº", "å…‰ä¼ç”µç«™", "ç”µç«™", "åœºç«™"]
EXCLUDE_KEYWORDS = ["è®¡é‡ç”µé‡", "è®¡é‡é‡", "ç”µé‡", "ç”µä»·", "ç”µè´¹", "åˆè®¡", "å°è®¡"]
# ä»…é’ˆå¯¹åœºç«™åç§°çš„æ•°å­—è¿‡æ»¤ï¼ˆä¸å½±å“ç¼–ç /æ•°æ®ï¼‰
STATION_NUMBER_PATTERN = re.compile(r'\s+\d+\.?\d*')  # åŒ¹é…â€œ 1167.741â€è¿™ç±»åœºç«™åçš„æ•°å­—

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆæ¢å¤+ç²¾å‡†ä¼˜åŒ–ï¼‰ ----------------------
def remove_redundant_text(text):
    """æ¢å¤æ ¸å¿ƒï¼šä»…æ¸…ç†å†—ä½™æ–‡æœ¬/ä¹±ç ï¼Œä¸åˆ é™¤æ•°å­—ï¼ˆä¿ç•™ç¼–ç /ç”µé‡/ç”µè´¹ï¼‰"""
    if not text:
        return ""
    cleaned = str(text).strip()
    # 1. æ¸…ç†å†—ä½™å…³é”®è¯
    for keyword in REDUNDANT_KEYWORDS:
        if keyword not in ["æœºç»„", "ç”µé‡", "ç”µä»·", "ç”µè´¹"]:  # ä¿ç•™ä¸šåŠ¡å…³é”®è¯
            cleaned = cleaned.replace(keyword, "")
    # 2. æ¸…ç†å•ä¸ªä¹±ç å­—ç¬¦ï¼ˆä¸åˆ æ•°å­—ï¼‰
    single_watermarks = ["ä¾", "å…°", "å", "åˆ", "å¿", "ç”µ", "åŠ›", "å‘", "é™", "å¸"]
    for char in single_watermarks:
        cleaned = cleaned.replace(char, "")
    # 3. æ¸…ç†ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™æ•°å­—å’Œç¼–ç ï¼‰
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.\-\:\s]', '', cleaned)
    return cleaned.strip()

def clean_station_name(station_name):
    """ç²¾å‡†ï¼šä»…æ¸…ç†åœºç«™åç§°ä¸­çš„æ•°å­—/å†—ä½™ï¼Œä¸å½±å“å…¶ä»–å­—æ®µ"""
    if not station_name or station_name == "æœªçŸ¥åœºç«™":
        return "æœªçŸ¥åœºç«™"
    # 1. å‰”é™¤åœºç«™åç§°åçš„æ•°å­—ï¼ˆå¦‚â€œåŒAé£åœº 1167.741â€â†’â€œåŒAé£åœºâ€ï¼‰
    cleaned = STATION_NUMBER_PATTERN.sub('', station_name)
    # 2. æ ‡å‡†åŒ–åœºç«™åç§°
    standard_map = {
        "åŒå‘Aé£ç”µåœº": "åŒå‘Aé£ç”µåœº",
        "åŒå‘Bé£ç”µåœº": "åŒå‘Bé£ç”µåœº",
        "åŒAé£åœº": "åŒå‘Aé£ç”µåœº",
        "åŒBé£åœº": "åŒå‘Bé£ç”µåœº",
        "åŒå‘A": "åŒå‘Aé£ç”µåœº",
        "åŒå‘B": "åŒå‘Bé£ç”µåœº",
        "åŒA": "åŒå‘Aé£ç”µåœº",
        "åŒB": "åŒå‘Bé£ç”µåœº"
    }
    for variant, standard in standard_map.items():
        if variant in cleaned:
            return standard
    # 3. å…œåº•ä¿ç•™æœ‰æ•ˆåœºç«™å
    for type_key in STATION_TYPE_KEYWORDS:
        if type_key in cleaned:
            match = re.search(r'([^ï¼Œã€‚\n]+' + type_key + ')', cleaned)
            if match:
                return match.group(1).strip()
    return cleaned.strip()

def extract_station_from_text(pdf_text):
    """æ¢å¤ï¼šä»æ–‡æœ¬æå–åœºç«™ï¼Œä»…æ¸…ç†åœºç«™åçš„æ•°å­—"""
    clean_text = remove_redundant_text(pdf_text)
    station_patterns = [
        r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\n]+é£ç”µåœº|[^ï¼Œã€‚\n]+é£åœº)',
        r'æœºç»„åç§°[:ï¼š\s]*([^ï¼Œã€‚\n]+é£ç”µåœº|[^ï¼Œã€‚\n]+é£åœº)',
        r'(åŒå‘[AB]é£ç”µåœº|åŒ[AB]é£åœº|åŒå‘[AB]|åŒ[AB])'
    ]
    for pattern in station_patterns:
        match = re.search(pattern, clean_text)
        if match:
            raw_name = match.group(1).strip()
            return clean_station_name(raw_name)
    # å…œåº•
    for type_key in STATION_TYPE_KEYWORDS:
        match = re.search(r'([^ï¼Œã€‚\n]+' + type_key + ')', clean_text)
        if match:
            raw_name = match.group(1).strip()
            return clean_station_name(raw_name)
    return "æœªçŸ¥åœºç«™"

def extract_station_from_filename(file_name):
    """æ¢å¤ï¼šä»æ–‡ä»¶åæå–åœºç«™ï¼Œä¸åˆ é™¤æ•°å­—ï¼ˆé¿å…å½±å“æ—¥æœŸï¼‰"""
    if not file_name:
        return "æœªçŸ¥åœºç«™"
    name_patterns = [
        r'(åŒå‘[AB]é£ç”µåœº|åŒ[AB]é£åœº|åŒå‘[AB]|åŒ[AB])',
        r'([^_]+åŒå‘[AB][^_]+)',
        r'([^_]+åŒ[AB][^_]+)'
    ]
    for pattern in name_patterns:
        match = re.search(pattern, file_name)
        if match:
            raw_name = match.group(1).strip()
            return clean_station_name(raw_name)
    return "æœªçŸ¥åœºç«™"

def safe_convert_to_numeric(value, data_type=""):
    """å®Œå…¨æ¢å¤ï¼šæ­£å¸¸è½¬æ¢æ•°å­—ï¼ˆç¼–ç /ç”µé‡/ç”µè´¹ï¼‰"""
    if value is None or pd.isna(value) or value == '':
        return None
    val_str = remove_redundant_text(value)
    # ä¿ç•™ç§‘ç›®ç¼–ç ï¼ˆ10ä½æ•°å­—ï¼‰ä¸è¿‡æ»¤
    if re.match(r'^\d{9,10}$', val_str):
        return val_str  # ç¼–ç è¿”å›å­—ç¬¦ä¸²ï¼Œé¿å…ä¸¢å¤±å‰å¯¼0
    if val_str in ['-', '.', '', 'â€”', 'â€”â€”']:
        return None
    try:
        cleaned = re.sub(r'[^\d\-\.]', '', val_str.replace('ï¼Œ', ',').replace('ã€‚', '.'))
        if not cleaned or cleaned in ['-', '.', '-.' , '-.']:
            return None
        num = float(cleaned)
        if data_type in DATA_RULES:
            rule = DATA_RULES[data_type]
            if num < rule["min"] or num > rule["max"]:
                return None
        return num
    except (ValueError, TypeError):
        return None

def extract_company_info(pdf_text, file_name):
    """æ¢å¤ï¼šæ­£å¸¸æå–å…¬å¸åç§°"""
    clean_text = remove_redundant_text(pdf_text)
    company_name = "æœªçŸ¥å‘ç”µå…¬å¸"
    company_match = re.search(r'å…¬å¸åç§°[:ï¼š]\s*([^ï¼Œã€‚\n]+å…¬å¸)', clean_text)
    if company_match:
        company_name = company_match.group(1).strip()
    else:
        company_match = re.search(r'([^_]+å…¬å¸|[^_]+å‘ç”µ)', file_name)
        if company_match:
            company_name = company_match.group(1).strip()
    return company_name

def extract_clear_date(pdf_text, file_name):
    """æ¢å¤ï¼šæ­£å¸¸æå–æ—¥æœŸ"""
    raw_text = str(pdf_text).strip()
    date = None
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'ç»“ç®—æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'(\d{4}-\d{1,2}-\d{1,2})'
    ]
    for pattern in date_patterns:
        match = re.search(pattern, raw_text)
        if match:
            date_str = match.group(1).strip()
            if "å¹´" in date_str:
                date_str = date_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            date = date_str
            break
    if not date:
        date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2}|\d{8})', file_name)
        if date_match:
            date_str = date_match.group(1)
            if len(date_str) == 8:
                date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date = date_str
    return date

def split_double_station_tables(all_tables, pdf_text, file_name):
    """æ¢å¤ï¼šæ­£å¸¸æ‹†åˆ†åŒåœºç«™è¡¨æ ¼ï¼Œä¸ä¸¢å¤±æ•°æ®è¡Œ"""
    clean_text = remove_redundant_text(pdf_text)
    merged_rows = []
    # æ¢å¤ï¼šå®Œæ•´åˆå¹¶æ‰€æœ‰è¡¨æ ¼è¡Œï¼ˆä¸å‰”é™¤ä»»ä½•å«æ•°å­—çš„è¡Œï¼‰
    for table in all_tables:
        if not table:
            continue
        for row in table:
            cleaned_row = [remove_redundant_text(cell) for cell in row]
            if any(cell.strip() != "" for cell in cleaned_row):
                merged_rows.append(cleaned_row)
    
    if not merged_rows:
        text_station = extract_station_from_text(pdf_text)
        return [(text_station, [])] if text_station != "æœªçŸ¥åœºç«™" else [(extract_station_from_filename(file_name), [])]
    
    station_segments = []
    current_segment = []
    current_station = extract_station_from_text(pdf_text)
    
    # æ¢å¤ï¼šæ­£å¸¸è¯†åˆ«åœºç«™åˆ‡æ¢ï¼Œä¸é—æ¼æ•°æ®
    for row in merged_rows:
        row_str = ''.join(row).replace(" ", "")
        has_station_key = any(keyword in row_str for keyword in STATION_SPLIT_KEYWORDS)
        
        if has_station_key:
            if current_segment:
                cleaned_station = clean_station_name(current_station)
                station_segments.append((cleaned_station, current_segment))
            # æå–å½“å‰åœºç«™
            row_text = ' '.join(row)
            station_match = re.search(r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\n]+)', row_text) or re.search(r'(åŒå‘[AB]|åŒ[AB])', row_text)
            if station_match:
                current_station = station_match.group(1).strip()
            current_segment = [row]
        else:
            current_segment.append(row)
    
    # ä¿å­˜æœ€åä¸€æ®µ
    if current_segment:
        cleaned_station = clean_station_name(current_station)
        if cleaned_station == "æœªçŸ¥åœºç«™":
            cleaned_station = extract_station_from_filename(file_name)
        station_segments.append((cleaned_station, current_segment))
    
    # æ¢å¤ï¼šè¿‡æ»¤æ— æ•ˆæ®µä½†ä¿ç•™æœ‰æ•ˆæ•°æ®
    valid_segments = [(s, seg) for s, seg in station_segments if len(seg) >= 2 or s != "æœªçŸ¥åœºç«™"]
    return valid_segments if valid_segments else [(extract_station_from_filename(file_name), merged_rows)]

def get_trade_name(trade_code, trade_text):
    """æ¢å¤ï¼šæ­£å¸¸åŒ¹é…ç§‘ç›®åç§°"""
    if trade_code in TRADE_CODE_MAP:
        return TRADE_CODE_MAP[trade_code]
    for key, name in TRADE_KEYWORDS.items():
        if key in trade_text:
            return name
    return "æœªè¯†åˆ«ç§‘ç›®"

def parse_single_station_data(station_name, table_segment, company_name, clear_date):
    """æ¢å¤ï¼šå®Œæ•´è§£æç§‘ç›®æ•°æ®ï¼Œä¸ä¸¢å¤±ä»»ä½•æœ‰æ•ˆç§‘ç›®"""
    trade_records = []
    valid_rows = []
    
    # æ¢å¤ï¼šæ­£å¸¸è¿‡æ»¤æ— æ•ˆè¡Œï¼Œä¿ç•™å«ç¼–ç /æ•°æ®çš„è¡Œ
    for row in table_segment:
        if not row:
            continue
        row_clean = [remove_redundant_text(cell) for cell in row]
        row_str = ''.join(row_clean).replace(" ", "")
        is_empty = all(cell == '' for cell in row_clean)
        is_header = any(keyword in row_str for keyword in ["ç§‘ç›®ç¼–ç ", "ç»“ç®—ç±»å‹", "ç”µé‡", "ç”µä»·", "ç”µè´¹"])
        
        # æ¢å¤ï¼šæ­£å¸¸åˆ¤æ–­æœ‰æ•ˆè¡Œï¼ˆå«ç¼–ç /å…³é”®è¯/æ•°æ®ï¼‰
        has_code = any(re.match(r'^\d{9,10}$', cell.replace(" ", "")) for cell in row_clean)
        has_trade_key = any(key in row_str for key in TRADE_KEYWORDS.keys())
        has_valid_data = any(safe_convert_to_numeric(cell) is not None for cell in row_clean if cell not in ['', '-'])
        
        if (has_code or has_trade_key or has_valid_data) and not is_empty and not is_header:
            valid_rows.append(row_clean)
    
    if len(valid_rows) < 2:
        return trade_records
    
    # æ¢å¤ï¼šæ­£å¸¸å®šä½åˆ—ï¼ˆç¼–ç /åç§°/ç”µé‡/ç”µä»·/ç”µè´¹ï¼‰
    cols = {"code": -1, "name": -1, "qty": -1, "price": -1, "fee": -1}
    header_idx = -1
    for idx, row in enumerate(valid_rows[:3]):
        row_str = ''.join(row).replace(" ", "")
        if "ç»“ç®—ç±»å‹" in row_str:
            header_idx = idx
            break
    header_idx = header_idx if header_idx != -1 else 0
    header_row = valid_rows[header_idx]
    
    for col_idx, cell in enumerate(header_row):
        cell_clean = remove_redundant_text(cell).lower()
        if "ç¼–ç " in cell_clean:
            cols["code"] = col_idx
        elif "ç»“ç®—ç±»å‹" in cell_clean:
            cols["name"] = col_idx
        elif "ç”µé‡" in cell_clean and "ä»·" not in cell_clean:
            cols["qty"] = col_idx
        elif "ç”µä»·" in cell_clean or "å•ä»·" in cell_clean:
            cols["price"] = col_idx
        elif "ç”µè´¹" in cell_clean or "é‡‘é¢" in cell_clean:
            cols["fee"] = col_idx
    
    # æ¢å¤ï¼šå…œåº•åˆ—é¡ºåºï¼Œç¡®ä¿æ•°æ®èƒ½æå–
    if any(v == -1 for v in cols.values()) and len(header_row) >= 5:
        cols = {"code": 0, "name": 1, "qty": 2, "price": 3, "fee": 4}
    
    # æ¢å¤ï¼šå®Œæ•´è§£ææ¯ä¸€è¡Œæ•°æ®
    data_start_idx = header_idx + 1
    for row_idx in range(data_start_idx, len(valid_rows)):
        row = valid_rows[row_idx]
        row_str = ''.join(row).replace(" ", "")
        is_subtotal = "å°è®¡" in row_str
        
        if "åˆè®¡" in row_str and not is_subtotal:
            continue
        
        # æ¢å¤ï¼šæ­£å¸¸æå–ç¼–ç å’Œåç§°
        trade_code = row[cols["code"]].strip().replace(" ", "") if (cols["code"] < len(row)) else ""
        trade_text = row[cols["name"]].strip() if (cols["name"] < len(row)) else ""
        trade_name = get_trade_name(trade_code, trade_text)
        
        if trade_name == "æœªè¯†åˆ«ç§‘ç›®" and not is_subtotal:
            continue
        
        # æ¢å¤ï¼šæ­£å¸¸æå–å°è®¡æ•°æ®
        if is_subtotal:
            subtotal_qty = None
            subtotal_fee = None
            nums = [safe_convert_to_numeric(cell, "ç”µé‡(å…†ç“¦æ—¶)") for cell in row if isinstance(safe_convert_to_numeric(cell), (int, float))]
            fee_nums = [safe_convert_to_numeric(cell, "ç”µè´¹(å…ƒ)") for cell in row if isinstance(safe_convert_to_numeric(cell), (int, float))]
            subtotal_qty = nums[0] if nums else None
            subtotal_fee = fee_nums[-1] if fee_nums else None
            trade_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "å½“æ—¥å°è®¡",
                "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
                "ç”µé‡(å…†ç“¦æ—¶)": subtotal_qty,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": subtotal_fee
            })
            continue
        
        # æ¢å¤ï¼šæ­£å¸¸æå–ç§‘ç›®æ•°æ®
        quantity = safe_convert_to_numeric(row[cols["qty"]], "ç”µé‡(å…†ç“¦æ—¶)") if (cols["qty"] < len(row)) else None
        price = safe_convert_to_numeric(row[cols["price"]], "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)") if (cols["price"] < len(row)) else None
        fee = safe_convert_to_numeric(row[cols["fee"]], "ç”µè´¹(å…ƒ)") if (cols["fee"] < len(row)) else None
        
        # æ¢å¤ï¼šç‰¹æ®Šç§‘ç›®å¤„ç†
        if "é˜»å¡è´¹ç”¨" in trade_name or "ä»·å·®è´¹ç”¨" in trade_name or "è¾…åŠ©æœåŠ¡" in trade_name or "åå·®è€ƒæ ¸" in trade_name:
            quantity = None
            price = None
        
        if quantity is None and fee is None:
            continue
        
        trade_records.append({
            "å…¬å¸åç§°": company_name,
            "åœºç«™åç§°": station_name,
            "æ¸…åˆ†æ—¥æœŸ": clear_date,
            "ç§‘ç›®åç§°": trade_name,
            "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
            "ç”µé‡(å…†ç“¦æ—¶)": quantity,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": price,
            "ç”µè´¹(å…ƒ)": fee
        })
    
    return trade_records

# ---------------------- ä¸»è§£æå‡½æ•°ï¼ˆå®Œå…¨æ¢å¤åŠŸèƒ½ï¼‰ ----------------------
def parse_pdf_final(file_obj, file_name):
    try:
        file_obj.seek(0)
        file_bytes = BytesIO(file_obj.read())
        file_bytes.seek(0)
        
        # æ¢å¤ï¼šå®Œæ•´æå–PDFæ–‡æœ¬å’Œè¡¨æ ¼
        all_text = ""
        all_tables = []
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                all_text += text + "\n"
                tables = page.extract_tables({
                    "vertical_strategy": "lines_strict",
                    "horizontal_strategy": "lines_strict",
                    "snap_tolerance": 1,
                    "join_tolerance": 1,
                    "edge_min_length": 3
                })
                all_tables.extend(tables)
        
        # æ¢å¤ï¼šæ­£å¸¸æå–åŸºç¡€ä¿¡æ¯
        company_name = extract_company_info(all_text, file_name)
        clear_date = extract_clear_date(all_text, file_name)
        station_segments = split_double_station_tables(all_tables, all_text, file_name)
        
        # æ¢å¤ï¼šæ­£å¸¸è§£ææ¯ä¸ªåœºç«™æ•°æ®
        all_records = []
        for station_name, table_segment in station_segments:
            final_station = clean_station_name(station_name)
            station_data = parse_single_station_data(final_station, table_segment, company_name, clear_date)
            all_records.extend(station_data)
        
        # æ¢å¤ï¼šå»é‡ä½†ä¿ç•™æ‰€æœ‰æœ‰æ•ˆæ•°æ®
        unique_records = []
        seen_keys = set()
        for rec in all_records:
            key = f"{rec['åœºç«™åç§°']}_{rec['ç§‘ç›®åç§°']}_{rec['åŸå§‹ç§‘ç›®ç¼–ç ']}"
            if key not in seen_keys:
                seen_keys.add(key)
                unique_records.append(rec)
        
        # æ¢å¤ï¼šå…œåº•é¿å…ç©ºç»“æœ
        if not unique_records:
            fallback_station = extract_station_from_filename(file_name)
            unique_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": fallback_station,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "æ— æœ‰æ•ˆæ•°æ®",
                "åŸå§‹ç§‘ç›®ç¼–ç ": "",
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
                "ç”µé‡(å…†ç“¦æ—¶)": None,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": None
            })
        
        return unique_records
    
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {str(e)}")
        fallback_station = extract_station_from_filename(file_name)
        return [{
            "å…¬å¸åç§°": "æœªçŸ¥å‘ç”µå…¬å¸",
            "åœºç«™åç§°": fallback_station,
            "æ¸…åˆ†æ—¥æœŸ": None,
            "ç§‘ç›®åç§°": "è§£æå¤±è´¥",
            "åŸå§‹ç§‘ç›®ç¼–ç ": "",
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
            "ç”µé‡(å…†ç“¦æ—¶)": None,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
            "ç”µè´¹(å…ƒ)": None
        }]

# ---------------------- Streamlitåº”ç”¨ï¼ˆæ¢å¤å®Œæ•´åŠŸèƒ½ï¼‰ ----------------------
def main():
    st.set_page_config(page_title="é€šç”¨æ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·ï¼ˆåŠŸèƒ½æ¢å¤ç‰ˆï¼‰", layout="wide")
    
    st.title("ğŸ“Š é€šç”¨ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆåŒåœºç«™å®Œæ•´ç‰ˆï¼‰")
    st.markdown("**æ ¸å¿ƒï¼šæ¢å¤æ‰€æœ‰æå–é€»è¾‘ | åœºç«™åç§°ç²¾å‡†å»å†—ä½™ | ç§‘ç›®/æ•°æ®æ— ä¸¢å¤±**")
    st.divider()
    
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆæ”¯æŒåŒåœºç«™/å¤šé¡µé¢ï¼‰",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸš€ å¼€å§‹æ‰¹é‡æå–", type="primary"):
        st.divider()
        st.subheader("âš™ï¸ å¤„ç†è¿›åº¦")
        
        all_results = []
        progress_bar = st.progress(0)
        
        for idx, file in enumerate(uploaded_files):
            st.write(f"æ­£åœ¨å¤„ç†ï¼š{file.name}")
            file_results = parse_pdf_final(file, file.name)
            all_results.extend(file_results)
            progress_bar.progress((idx + 1) / len(uploaded_files))
            file.close()
        
        progress_bar.empty()
        
        # æ¢å¤ï¼šæ­£å¸¸æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
        df = pd.DataFrame(all_results).fillna("")
        display_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "åŸå§‹ç§‘ç›®ç¼–ç ", "åŸå§‹ç§‘ç›®æ–‡æœ¬", "ç”µé‡(å…†ç“¦æ—¶)", 
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_display = df[[col for col in display_cols if col in df.columns]]
        
        # æ¢å¤ï¼šé«˜äº®æ˜¾ç¤º
        st.subheader("ğŸ“ˆ æ‰¹é‡æå–ç»“æœï¼ˆåŠŸèƒ½å®Œå…¨æ¢å¤ï¼‰")
        def highlight_rows(row):
            if row["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                return ["background-color: #e6f3ff"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Aé£ç”µåœº":
                return ["background-color: #f0fff4"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Bé£ç”µåœº":
                return ["background-color: #fff8f0"] * len(row)
            else:
                return [""] * len(row)
        styled_df = df_display.style.apply(highlight_rows, axis=1)
        st.dataframe(styled_df, use_container_width=True)
        
        # æ¢å¤ï¼šæ­£å¸¸ç»Ÿè®¡
        total_stations = df["åœºç«™åç§°"].nunique()
        total_trades = len(df[(df["ç§‘ç›®åç§°"] != "å½“æ—¥å°è®¡") & (df["ç§‘ç›®åç§°"] != "æ— æœ‰æ•ˆæ•°æ®") & (df["ç§‘ç›®åç§°"] != "è§£æå¤±è´¥")])
        subtotal_count = len(df[df["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡"])
        st.info(f"**ç»Ÿè®¡ï¼š** è¦†ç›–åœºç«™ {total_stations} ä¸ª | æœ‰æ•ˆç§‘ç›® {total_trades} ä¸ª | å°è®¡è¡Œ {subtotal_count} ä¸ª")
        
        # æ¢å¤ï¼šæ­£å¸¸ä¸‹è½½Excel
        download_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "ç”µé‡(å…†ç“¦æ—¶)", "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_download = df[[col for col in download_cols if col in df.columns]]
        
        output = BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_download.to_excel(writer, index=False, sheet_name="å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®")
            ws = writer.sheets["å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®"]
            light_blue = PatternFill(start_color="E6F3FF", end_color="E6F3FF", fill_type="solid")
            for row in range(2, len(df_download) + 2):
                if df_download.iloc[row-2]["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                    for col in range(1, len(df_download.columns) + 1):
                        ws.cell(row=row, column=col).fill = light_blue
        
        output.seek(0)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excelï¼ˆä¸å«åŸå§‹ç¼–ç /æ–‡æœ¬ï¼‰",
            data=output,
            file_name=f"å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        st.success("âœ… æå–å®Œæˆï¼æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å·²æ¢å¤ï¼Œåœºç«™åç§°æ— å†—ä½™ï¼Œç§‘ç›®/æ•°æ®æ— ä¸¢å¤±")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ åŒåœºç«™ï¼ˆå¦‚åŒå‘A/Bé£ç”µåœºï¼‰çš„ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•PDF")

if __name__ == "__main__":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    main()
