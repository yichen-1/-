import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO
import sys
import os
from openpyxl.styles import PatternFill

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆæ–°å¢é˜»å¡/ä»·å·®è´¹ç”¨ä¸“å±é…ç½®ï¼‰ ----------------------
REDUNDANT_KEYWORDS = [
    "å†…éƒ¨ä½¿ç”¨", "CONFIDENTIAL", "è‰ç¨¿", "ç°è´§è¯•ç»“ç®—æœŸé—´", "æ—¥æ¸…åˆ†å•æ®",
    "å…¬å¸åç§°", "ç¼–å·ï¼š", "å•ä½ï¼š", "æ¸…åˆ†æ—¥æœŸ", "åˆè®¡ç”µé‡", "åˆè®¡ç”µè´¹",
    "ç”µèƒ½é‡ç”µè´¹", "å®¡æ‰¹ï¼š", "å®¡æ ¸ï¼š", "ç¼–åˆ¶ï¼š", "åŠ ç›–ç”µå­ç­¾ç« ",
    "ylxxhfd", "yxxchfd", "ä¾å…°å¿ååˆé£åŠ›å‘ç”µæœ‰é™å…¬å¸", "ä¾", "ä¾å…°", "ååˆ",
    "å¿", "é£åŠ›å‘ç”µ", "æœ‰é™å…¬å¸", "å¸"
]
TRADE_CODE_MAP = {
    "0101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "0101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "0101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "0101040203": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040301": "é€è¾½å®äº¤æ˜“",
    "0101040321": "é€ååŒ—äº¤æ˜“", 
    "0101040322": "é€å±±ä¸œäº¤æ˜“",
    "0101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "0102020101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "0102020301": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "0102010101": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "0102010201": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "0202030001": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",  # é‡ç‚¹ç§‘ç›®
    "0202030002": "çœé—´çœå†…ä»·å·®è´¹ç”¨",   # é‡ç‚¹ç§‘ç›®
    "0101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "0101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "0101100101": "åå·®è€ƒæ ¸è´¹ç”¨",
    "0101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101100001": "æ—¥èåˆäº¤æ˜“",
    "101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "101020201": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040202": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101040204": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "101100001": "æ—¥èåˆäº¤æ˜“",
    "101040330": "é€æµ™æ±Ÿäº¤æ˜“",
    "101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "101100101": "åå·®è€ƒæ ¸è´¹ç”¨"
}
TRADE_KEYWORDS = {
    "ä¼˜å…ˆå‘ç”µ": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "ä»£ç†è´­ç”µ": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“",
    "ç›´æ¥äº¤æ˜“": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "ç»¿è‰²ç”µåŠ›": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æ±Ÿè‹": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€æµ™æ±Ÿ": "é€æµ™æ±Ÿäº¤æ˜“",
    "é€æµ™æ±Ÿçœé—´": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€ä¸Šæµ·": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "é€è¾½å®": "é€è¾½å®äº¤æ˜“",
    "é€ååŒ—": "é€ååŒ—äº¤æ˜“",
    "é€å±±ä¸œ": "é€å±±ä¸œäº¤æ˜“",
    "æ—¥èåˆ": "æ—¥èåˆäº¤æ˜“",
    "ç°è´§æ—¥å‰": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "ç°è´§å®æ—¶": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "é˜»å¡è´¹ç”¨": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",       # å¼ºåŒ–å…³é”®è¯
    "ä¸­é•¿æœŸåˆçº¦é˜»å¡": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨", # æ–°å¢å˜ä½“
    "ä»·å·®è´¹ç”¨": "çœé—´çœå†…ä»·å·®è´¹ç”¨",        # å¼ºåŒ–å…³é”®è¯
    "çœé—´çœå†…ä»·å·®": "çœé—´çœå†…ä»·å·®è´¹ç”¨",     # æ–°å¢å˜ä½“
    "ç°è´§ç»“ç®—": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "è¾…åŠ©æœåŠ¡": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "åå·®è€ƒæ ¸": "åå·®è€ƒæ ¸è´¹ç”¨"
}
# è°ƒæ•´æ•°æ®è§„åˆ™ï¼šå…è®¸é˜»å¡/ä»·å·®è´¹ç”¨çš„ç”µè´¹ä¸ºè´Ÿæ•°ä¸”èŒƒå›´æ›´å¹¿
DATA_RULES = {
    "ç”µé‡(å…†ç“¦æ—¶)": {"min": -1000, "max": 5000},
    "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": {"min": 0, "max": 2000},
    "ç”µè´¹(å…ƒ)": {"min": -1000000, "max": 10000000}  # æ‰©å¤§è´Ÿæ•°èŒƒå›´
}
# 1. æ–°å¢æ™¶ç››å…‰ä¼ç«™å˜ä½“ï¼Œè¦†ç›–â€œæ™¶ç››å…‰ä¼ç«™â€â†’â€œæ™¶ç››å…‰ä¼ç”µç«™â€
STATION_CORE_NAMES = [
    "åŒå‘Aé£ç”µåœº", "åŒå‘Bé£ç”µåœº", 
    "æ™¶ç››å…‰ä¼ç”µç«™", "æ™¶ç››å…‰ä¼ç«™",  # æ–°å¢æ™¶ç››å˜ä½“
    "æ™¶ç››å…‰ä¼",  # æç®€å˜ä½“
    "åŒAé£åœº", "åŒBé£åœº", "åŒå‘A", "åŒå‘B"
]
STATION_SPLIT_KEYWORDS = ["æœºç»„", "æœºç»„åç§°", "åŒå‘A", "åŒå‘B", "æ™¶ç››", "é£åœº", "é£ç”µåœº", "å…‰ä¼"]
# 2. æ–°å¢â€œå…‰ä¼ç«™â€ç±»å‹å…³é”®è¯ï¼Œä¾¿äºè¯†åˆ«è¡¥å…¨
STATION_TYPE_KEYWORDS = ["é£ç”µåœº", "é£åœº", "å…‰ä¼ç”µç«™", "å…‰ä¼ç«™", "ç”µç«™", "åœºç«™"]
# 3. ç¡®ä¿â€œè®¡é‡é‡â€åœ¨æœ€å‰ï¼Œä¼˜å…ˆæˆªæ–­
EXCLUDE_KEYWORDS = ["è®¡é‡é‡", "è®¡é‡ç”µé‡", "ç”µé‡", "ç”µä»·", "ç”µè´¹", "åˆè®¡", "å°è®¡"]
# ä»…é’ˆå¯¹åœºç«™åç§°çš„æ•°å­—/å†—ä½™è¿‡æ»¤
STATION_NUMBER_PATTERN = re.compile(r'\s+\d+\.?\d*')
# åœºç«™åç§°å†—ä½™è¯æ­£åˆ™ï¼ˆåŒ¹é…â€œ è®¡é‡é‡â€â€œ è®¡é‡ç”µé‡â€ï¼‰
STATION_REDUNDANT_PATTERN = re.compile(r'\s+(è®¡é‡é‡|è®¡é‡ç”µé‡|ç”µé‡|ç”µä»·|ç”µè´¹)')

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆé‡ç‚¹ä¿®å¤é˜»å¡/ä»·å·®è´¹ç”¨æå–ï¼‰ ----------------------
def remove_redundant_text(text):
    if not text:
        return ""
    cleaned = str(text).strip()
    for keyword in REDUNDANT_KEYWORDS:
        if keyword not in ["æœºç»„", "ç”µé‡", "ç”µä»·", "ç”µè´¹", "æ™¶ç››"]:  # ä¿ç•™æ™¶ç››å…³é”®è¯
            cleaned = cleaned.replace(keyword, "")
    single_watermarks = ["ä¾", "å…°", "å", "åˆ", "å¿", "ç”µ", "åŠ›", "å‘", "é™", "å¸"]
    for char in single_watermarks:
        cleaned = cleaned.replace(char, "")
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\.\-\:\s]', '', cleaned)
    return cleaned.strip()

def clean_station_name(station_name):
    """æ ¸å¿ƒä¿®å¤ï¼šæ™¶ç››å…‰ä¼ç«™è¡¥å…¨+å†—ä½™è¯æˆªæ–­"""
    if not station_name or station_name == "æœªçŸ¥åœºç«™":
        return "æœªçŸ¥åœºç«™"
    # 1. å…ˆå‰”é™¤åœºç«™åç§°åçš„å†—ä½™è¯ï¼ˆå¦‚â€œæ™¶ç››å…‰ä¼ç«™ è®¡é‡é‡â€â†’â€œæ™¶ç››å…‰ä¼ç«™â€ï¼‰
    cleaned = STATION_REDUNDANT_PATTERN.sub('', station_name)
    # 2. å‰”é™¤æ•°å­—ï¼ˆå¦‚â€œæ™¶ç››å…‰ä¼ç«™ 123â€â†’â€œæ™¶ç››å…‰ä¼ç«™â€ï¼‰
    cleaned = STATION_NUMBER_PATTERN.sub('', cleaned)
    # 3. æ ‡å‡†åŒ–åç§°ï¼šè¡¥å…¨â€œæ™¶ç››å…‰ä¼ç«™â€â†’â€œæ™¶ç››å…‰ä¼ç”µç«™â€
    standard_map = {
        "åŒå‘Aé£ç”µåœº": "åŒå‘Aé£ç”µåœº",
        "åŒå‘Bé£ç”µåœº": "åŒå‘Bé£ç”µåœº",
        "åŒAé£åœº": "åŒå‘Aé£ç”µåœº",
        "åŒBé£åœº": "åŒå‘Bé£ç”µåœº",
        "åŒå‘A": "åŒå‘Aé£ç”µåœº",
        "åŒå‘B": "åŒå‘Bé£ç”µåœº",
        "åŒA": "åŒå‘Aé£ç”µåœº",
        "åŒB": "åŒå‘Bé£ç”µåœº",
        # æ–°å¢æ™¶ç››å…‰ä¼ç«™æ ‡å‡†åŒ–æ˜ å°„
        "æ™¶ç››å…‰ä¼ç”µç«™": "æ™¶ç››å…‰ä¼ç”µç«™",
        "æ™¶ç››å…‰ä¼ç«™": "æ™¶ç››å…‰ä¼ç”µç«™",
        "æ™¶ç››å…‰ä¼": "æ™¶ç››å…‰ä¼ç”µç«™"
    }
    # ä¼˜å…ˆåŒ¹é…æ ‡å‡†åŒ–åç§°
    for variant, standard in standard_map.items():
        if variant in cleaned:
            return standard
    # 4. å…œåº•ï¼šè¡¥å…¨å…‰ä¼ç«™â†’å…‰ä¼ç”µç«™
    if "å…‰ä¼ç«™" in cleaned:
        return cleaned.replace("å…‰ä¼ç«™", "å…‰ä¼ç”µç«™")
    # 5. ä¿ç•™å…¶ä»–æœ‰æ•ˆåœºç«™å
    for type_key in STATION_TYPE_KEYWORDS:
        if type_key in cleaned:
            match = re.search(r'([^ï¼Œã€‚\n]+' + type_key + ')', cleaned)
            if match:
                return match.group(1).strip()
    return cleaned.strip()

def extract_station_from_text(pdf_text):
    """ä¿®å¤ï¼šä¼˜å…ˆæå–æ™¶ç››å…‰ä¼ç›¸å…³åç§°"""
    clean_text = remove_redundant_text(pdf_text)
    # 1. ä¼˜å…ˆåŒ¹é…æ™¶ç››å…‰ä¼ç›¸å…³ï¼ˆç¡®ä¿ä¸é—æ¼ï¼‰
    jingsheng_patterns = [
        r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\n]+æ™¶ç››[^ï¼Œã€‚\n]+å…‰ä¼ç”µç«™|[^ï¼Œã€‚\n]+æ™¶ç››[^ï¼Œã€‚\n]+å…‰ä¼ç«™)',
        r'æœºç»„åç§°[:ï¼š\s]*([^ï¼Œã€‚\n]+æ™¶ç››[^ï¼Œã€‚\n]+å…‰ä¼ç”µç«™|[^ï¼Œã€‚\n]+æ™¶ç››[^ï¼Œã€‚\n]+å…‰ä¼ç«™)',
        r'(æ™¶ç››å…‰ä¼ç”µç«™|æ™¶ç››å…‰ä¼ç«™|æ™¶ç››å…‰ä¼)'
    ]
    for pattern in jingsheng_patterns:
        match = re.search(pattern, clean_text)
        if match:
            raw_name = match.group(1).strip()
            return clean_station_name(raw_name)
    # 2. åŒ¹é…åŒå‘é£ç”µåœº
    shuangfa_patterns = [
        r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\n]+åŒå‘[^ï¼Œã€‚\n]+é£ç”µåœº|[^ï¼Œã€‚\n]+åŒå‘[^ï¼Œã€‚\n]+é£åœº)',
        r'(åŒå‘Aé£ç”µåœº|åŒå‘Bé£ç”µåœº|åŒAé£åœº|åŒBé£åœº|åŒå‘A|åŒå‘B)'
    ]
    for pattern in shuangfa_patterns:
        match = re.search(pattern, clean_text)
        if match:
            raw_name = match.group(1).strip()
            return clean_station_name(raw_name)
    # 3. å…œåº•æå–
    for type_key in STATION_TYPE_KEYWORDS:
        match = re.search(r'([^ï¼Œã€‚\n]+' + type_key + ')', clean_text)
        if match:
            raw_name = match.group(1).strip()
            return clean_station_name(raw_name)
    return "æœªçŸ¥åœºç«™"

def extract_station_from_filename(file_name):
    """ä¿®å¤ï¼šä»æ–‡ä»¶åæå–æ™¶ç››å…‰ä¼ç«™"""
    if not file_name:
        return "æœªçŸ¥åœºç«™"
    # 1. ä¼˜å…ˆåŒ¹é…æ™¶ç››å…‰ä¼
    if "æ™¶ç››" in file_name and "å…‰ä¼" in file_name:
        return "æ™¶ç››å…‰ä¼ç”µç«™"
    # 2. åŒ¹é…åŒå‘é£ç”µåœº
    if "åŒå‘" in file_name or "åŒA" in file_name or "åŒB" in file_name:
        shuangfa_match = re.search(r'(åŒå‘[AB]é£ç”µåœº|åŒ[AB]é£åœº|åŒå‘[AB]|åŒ[AB])', file_name)
        if shuangfa_match:
            return clean_station_name(shuangfa_match.group(1))
    # 3. å…œåº•
    for type_key in STATION_TYPE_KEYWORDS:
        match = re.search(r'([^_]+' + type_key + ')', file_name)
        if match:
            return clean_station_name(match.group(1))
    return "æœªçŸ¥åœºç«™"

def safe_convert_to_numeric(value, data_type=""):
    """ä¿®å¤ï¼šå¼ºåŒ–è´Ÿæ•°å¤„ç†ï¼Œé€‚é…é˜»å¡/ä»·å·®è´¹ç”¨çš„è´Ÿæ•°ç”µè´¹"""
    if value is None or pd.isna(value) or value == '':
        return None
    val_str = remove_redundant_text(value)
    if re.match(r'^\d{9,10}$', val_str):
        return val_str
    if val_str in ['-', '.', '', 'â€”', 'â€”â€”']:
        return None
    try:
        # ä¿®å¤ï¼šä¿ç•™è´Ÿå·ï¼Œæ­£ç¡®å¤„ç†è´Ÿæ•°
        cleaned = re.sub(r'[^\d\-\.]', '', val_str.replace('ï¼Œ', ',').replace('ã€‚', '.'))
        if not cleaned or cleaned in ['-', '.', '-.' , '-.']:
            return None
        num = float(cleaned)
        # ä¿®å¤ï¼šé˜»å¡/ä»·å·®è´¹ç”¨ä¸å—ç”µä»·æœ€å°å€¼é™åˆ¶
        if data_type in DATA_RULES:
            rule = DATA_RULES[data_type]
            # ç”µä»·ä»…å¯¹éé˜»å¡/ä»·å·®è´¹ç”¨åšæœ€å°å€¼é™åˆ¶
            if data_type == "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)" and "é˜»å¡" not in data_type and "ä»·å·®" not in data_type:
                if num < rule["min"] or num > rule["max"]:
                    return None
            else:
                if num < rule["min"] or num > rule["max"]:
                    return None
        return num
    except (ValueError, TypeError):
        return None

def extract_company_info(pdf_text, file_name):
    clean_text = remove_redundant_text(pdf_text)
    company_name = "æœªçŸ¥å‘ç”µå…¬å¸"
    company_match = re.search(r'å…¬å¸åç§°[:ï¼š]\s*([^ï¼Œã€‚\n]+å…¬å¸)', clean_text)
    if company_match:
        company_name = company_match.group(1).strip()
    else:
        company_match = re.search(r'([^_]+å…¬å¸|[^_]+å‘ç”µ)', file_name)
        if company_match:
            company_name = company_match.group(1).strip()
    return company_name

def extract_clear_date(pdf_text, file_name):
    raw_text = str(pdf_text).strip()
    date = None
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'ç»“ç®—æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2}|\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'(\d{4}-\d{1,2}-\d{1,2})'
    ]
    for pattern in date_patterns:
        match = re.search(pattern, raw_text)
        if match:
            date_str = match.group(1).strip()
            if "å¹´" in date_str:
                date_str = date_str.replace("å¹´", "-").replace("æœˆ", "-").replace("æ—¥", "")
            date = date_str
            break
    if not date:
        date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2}|\d{8})', file_name)
        if date_match:
            date_str = date_match.group(1)
            if len(date_str) == 8:
                date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date = date_str
    return date

def split_double_station_tables(all_tables, pdf_text, file_name):
    clean_text = remove_redundant_text(pdf_text)
    merged_rows = []
    for table in all_tables:
        if not table:
            continue
        for row in table:
            cleaned_row = [remove_redundant_text(cell) for cell in row]
            if any(cell.strip() != "" for cell in cleaned_row):
                merged_rows.append(cleaned_row)
    
    if not merged_rows:
        text_station = extract_station_from_text(pdf_text)
        return [(text_station, [])] if text_station != "æœªçŸ¥åœºç«™" else [(extract_station_from_filename(file_name), [])]
    
    station_segments = []
    current_segment = []
    current_station = extract_station_from_text(pdf_text)
    
    for row in merged_rows:
        row_str = ''.join(row).replace(" ", "")
        has_station_key = any(keyword in row_str for keyword in STATION_SPLIT_KEYWORDS)
        
        if has_station_key:
            if current_segment:
                cleaned_station = clean_station_name(current_station)
                station_segments.append((cleaned_station, current_segment))
            row_text = ' '.join(row)
            # ä¼˜å…ˆè¯†åˆ«æ™¶ç››å…‰ä¼
            if "æ™¶ç››" in row_text and "å…‰ä¼" in row_text:
                current_station = "æ™¶ç››å…‰ä¼ç”µç«™"
            else:
                station_match = re.search(r'æœºç»„[:ï¼š\s]*([^ï¼Œã€‚\n]+)', row_text) or re.search(r'(åŒå‘[AB]|åŒ[AB])', row_text)
                if station_match:
                    current_station = station_match.group(1).strip()
            current_segment = [row]
        else:
            # ä¿®å¤ï¼šä¿ç•™é˜»å¡/ä»·å·®è´¹ç”¨è¡Œ
            if "é˜»å¡è´¹ç”¨" in row_str or "ä»·å·®è´¹ç”¨" in row_str:
                current_segment.append(row)
            else:
                current_segment.append(row)
    
    if current_segment:
        cleaned_station = clean_station_name(current_station)
        if cleaned_station == "æœªçŸ¥åœºç«™":
            cleaned_station = extract_station_from_filename(file_name)
        station_segments.append((cleaned_station, current_segment))
    
    valid_segments = [(s, seg) for s, seg in station_segments if len(seg) >= 2 or s != "æœªçŸ¥åœºç«™"]
    return valid_segments if valid_segments else [(extract_station_from_filename(file_name), merged_rows)]

def get_trade_name(trade_code, trade_text):
    """ä¿®å¤ï¼šå¼ºåŒ–é˜»å¡/ä»·å·®è´¹ç”¨çš„åç§°åŒ¹é…"""
    # ä¼˜å…ˆåŒ¹é…ç¼–ç 
    if trade_code in TRADE_CODE_MAP:
        return TRADE_CODE_MAP[trade_code]
    # å¼ºåŒ–å…³é”®è¯åŒ¹é…
    trade_text_lower = trade_text.lower()
    for key, name in TRADE_KEYWORDS.items():
        if key in trade_text or key.lower() in trade_text_lower:
            return name
    # å…œåº•ï¼šç›´æ¥åŒ¹é…ç§‘ç›®åç§°
    if "é˜»å¡è´¹ç”¨" in trade_text:
        return "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨"
    if "ä»·å·®è´¹ç”¨" in trade_text:
        return "çœé—´çœå†…ä»·å·®è´¹ç”¨"
    return "æœªè¯†åˆ«ç§‘ç›®"

def parse_single_station_data(station_name, table_segment, company_name, clear_date):
    """æ ¸å¿ƒä¿®å¤ï¼šç¡®ä¿é˜»å¡/ä»·å·®è´¹ç”¨ä¸è¢«è·³è¿‡"""
    trade_records = []
    valid_rows = []
    
    for row in table_segment:
        if not row:
            continue
        row_clean = [remove_redundant_text(cell) for cell in row]
        row_str = ''.join(row_clean).replace(" ", "")
        is_empty = all(cell == '' for cell in row_clean)
        is_header = any(keyword in row_str for keyword in ["ç§‘ç›®ç¼–ç ", "ç»“ç®—ç±»å‹", "ç”µé‡", "ç”µä»·", "ç”µè´¹"])
        
        has_code = any(re.match(r'^\d{9,10}$', cell.replace(" ", "")) for cell in row_clean)
        has_trade_key = any(key in row_str for key in TRADE_KEYWORDS.keys())
        # ä¿®å¤ï¼šå•ç‹¬åˆ¤æ–­é˜»å¡/ä»·å·®è´¹ç”¨çš„æœ‰æ•ˆæ•°æ®
        has_block_spread = "é˜»å¡è´¹ç”¨" in row_str or "ä»·å·®è´¹ç”¨" in row_str
        has_valid_data = any(safe_convert_to_numeric(cell) is not None for cell in row_clean if cell not in ['', '-'])
        
        # ä¿®å¤ï¼šé˜»å¡/ä»·å·®è´¹ç”¨è¡Œç›´æ¥åˆ¤å®šä¸ºæœ‰æ•ˆ
        if (has_code or has_trade_key or has_valid_data or has_block_spread) and not is_empty and not is_header:
            valid_rows.append(row_clean)
    
    if len(valid_rows) < 1:  # æ”¾å®½è¡Œæ•°è¦æ±‚
        return trade_records
    
    cols = {"code": -1, "name": -1, "qty": -1, "price": -1, "fee": -1}
    header_idx = -1
    for idx, row in enumerate(valid_rows[:3]):
        row_str = ''.join(row).replace(" ", "")
        if "ç»“ç®—ç±»å‹" in row_str:
            header_idx = idx
            break
    header_idx = header_idx if header_idx != -1 else 0
    header_row = valid_rows[header_idx]
    
    for col_idx, cell in enumerate(header_row):
        cell_clean = remove_redundant_text(cell).lower()
        if "ç¼–ç " in cell_clean:
            cols["code"] = col_idx
        elif "ç»“ç®—ç±»å‹" in cell_clean:
            cols["name"] = col_idx
        elif "ç”µé‡" in cell_clean and "ä»·" not in cell_clean:
            cols["qty"] = col_idx
        elif "ç”µä»·" in cell_clean or "å•ä»·" in cell_clean:
            cols["price"] = col_idx
        elif "ç”µè´¹" in cell_clean or "é‡‘é¢" in cell_clean:
            cols["fee"] = col_idx
    
    if any(v == -1 for v in cols.values()) and len(header_row) >= 5:
        cols = {"code": 0, "name": 1, "qty": 2, "price": 3, "fee": 4}
    
    data_start_idx = header_idx + 1
    for row_idx in range(data_start_idx, len(valid_rows)):
        row = valid_rows[row_idx]
        row_str = ''.join(row).replace(" ", "")
        is_subtotal = "å°è®¡" in row_str
        
        if "åˆè®¡" in row_str and not is_subtotal:
            continue
        
        trade_code = row[cols["code"]].strip().replace(" ", "") if (cols["code"] < len(row)) else ""
        trade_text = row[cols["name"]].strip() if (cols["name"] < len(row)) else ""
        trade_name = get_trade_name(trade_code, trade_text)
        
        # ä¿®å¤ï¼šå³ä½¿æ˜¯æœªè¯†åˆ«ç§‘ç›®ï¼Œåªè¦æ˜¯é˜»å¡/ä»·å·®è´¹ç”¨ä¹Ÿä¿ç•™
        if trade_name == "æœªè¯†åˆ«ç§‘ç›®":
            if "é˜»å¡è´¹ç”¨" in trade_text:
                trade_name = "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨"
            elif "ä»·å·®è´¹ç”¨" in trade_text:
                trade_name = "çœé—´çœå†…ä»·å·®è´¹ç”¨"
            else:
                continue
        
        if is_subtotal:
            subtotal_qty = None
            subtotal_fee = None
            nums = [safe_convert_to_numeric(cell, "ç”µé‡(å…†ç“¦æ—¶)") for cell in row if isinstance(safe_convert_to_numeric(cell), (int, float))]
            fee_nums = [safe_convert_to_numeric(cell, "ç”µè´¹(å…ƒ)") for cell in row if isinstance(safe_convert_to_numeric(cell), (int, float))]
            subtotal_qty = nums[0] if nums else None
            subtotal_fee = fee_nums[-1] if fee_nums else None
            trade_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "å½“æ—¥å°è®¡",
                "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
                "ç”µé‡(å…†ç“¦æ—¶)": subtotal_qty,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": subtotal_fee
            })
            continue
        
        # æå–æ•°æ®
        quantity = safe_convert_to_numeric(row[cols["qty"]], "ç”µé‡(å…†ç“¦æ—¶)") if (cols["qty"] < len(row)) else None
        price = safe_convert_to_numeric(row[cols["price"]], "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)") if (cols["price"] < len(row)) else None
        fee = safe_convert_to_numeric(row[cols["fee"]], "ç”µè´¹(å…ƒ)") if (cols["fee"] < len(row)) else None
        
        # ä¿®å¤ï¼šé˜»å¡/ä»·å·®è´¹ç”¨ç‰¹æ®Šå¤„ç†
        if "é˜»å¡è´¹ç”¨" in trade_name or "ä»·å·®è´¹ç”¨" in trade_name:
            quantity = None
            price = None
            # å³ä½¿feeä¸ºNoneä¹Ÿä¿ç•™ï¼ˆé¿å…è·³è¿‡ï¼‰
            trade_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": trade_name,
                "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
                "ç”µé‡(å…†ç“¦æ—¶)": quantity,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": price,
                "ç”µè´¹(å…ƒ)": fee
            })
        else:
            # å…¶ä»–ç§‘ç›®åŸæœ‰é€»è¾‘
            if quantity is None and fee is None:
                continue
            trade_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": station_name,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": trade_name,
                "åŸå§‹ç§‘ç›®ç¼–ç ": trade_code,
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": trade_text,
                "ç”µé‡(å…†ç“¦æ—¶)": quantity,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": price,
                "ç”µè´¹(å…ƒ)": fee
            })
    
    return trade_records

# ---------------------- ä¸»è§£æå‡½æ•° ----------------------
def parse_pdf_final(file_obj, file_name):
    try:
        file_obj.seek(0)
        file_bytes = BytesIO(file_obj.read())
        file_bytes.seek(0)
        
        all_text = ""
        all_tables = []
        with pdfplumber.open(file_bytes) as pdf:
            for page in pdf.pages:
                text = page.extract_text() or ""
                all_text += text + "\n"
                tables = page.extract_tables({
                    "vertical_strategy": "lines_strict",
                    "horizontal_strategy": "lines_strict",
                    "snap_tolerance": 1,
                    "join_tolerance": 1,
                    "edge_min_length": 3
                })
                all_tables.extend(tables)
        
        company_name = extract_company_info(all_text, file_name)
        clear_date = extract_clear_date(all_text, file_name)
        station_segments = split_double_station_tables(all_tables, all_text, file_name)
        
        all_records = []
        for station_name, table_segment in station_segments:
            final_station = clean_station_name(station_name)
            station_data = parse_single_station_data(final_station, table_segment, company_name, clear_date)
            all_records.extend(station_data)
        
        unique_records = []
        seen_keys = set()
        for rec in all_records:
            key = f"{rec['åœºç«™åç§°']}_{rec['ç§‘ç›®åç§°']}_{rec['åŸå§‹ç§‘ç›®ç¼–ç ']}"
            if key not in seen_keys:
                seen_keys.add(key)
                unique_records.append(rec)
        
        if not unique_records:
            fallback_station = extract_station_from_filename(file_name)
            unique_records.append({
                "å…¬å¸åç§°": company_name,
                "åœºç«™åç§°": fallback_station,
                "æ¸…åˆ†æ—¥æœŸ": clear_date,
                "ç§‘ç›®åç§°": "æ— æœ‰æ•ˆæ•°æ®",
                "åŸå§‹ç§‘ç›®ç¼–ç ": "",
                "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
                "ç”µé‡(å…†ç“¦æ—¶)": None,
                "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
                "ç”µè´¹(å…ƒ)": None
            })
        
        return unique_records
    
    except Exception as e:
        st.error(f"PDFè§£æé”™è¯¯: {str(e)}")
        fallback_station = extract_station_from_filename(file_name)
        return [{
            "å…¬å¸åç§°": "æœªçŸ¥å‘ç”µå…¬å¸",
            "åœºç«™åç§°": fallback_station,
            "æ¸…åˆ†æ—¥æœŸ": None,
            "ç§‘ç›®åç§°": "è§£æå¤±è´¥",
            "åŸå§‹ç§‘ç›®ç¼–ç ": "",
            "åŸå§‹ç§‘ç›®æ–‡æœ¬": "",
            "ç”µé‡(å…†ç“¦æ—¶)": None,
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)": None,
            "ç”µè´¹(å…ƒ)": None
        }]

# ---------------------- Streamlitåº”ç”¨ ----------------------
def main():
    st.set_page_config(page_title="é€šç”¨æ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·ï¼ˆæ™¶ç››+é˜»å¡ä»·å·®ä¿®å¤ç‰ˆï¼‰", layout="wide")
    
    st.title("ğŸ“Š é€šç”¨ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆç²¾å‡†å®Œæ•´ç‰ˆï¼‰")
    st.markdown("**æ ¸å¿ƒä¿®å¤ï¼š1.æ™¶ç››å…‰ä¼ç«™â†’æ™¶ç››å…‰ä¼ç”µç«™ 2.é˜»å¡/ä»·å·®è´¹ç”¨ç²¾å‡†æå– 3.è´Ÿæ•°ç”µè´¹æ­£å¸¸è§£æ**")
    st.divider()
    
    uploaded_files = st.file_uploader(
        "ä¸Šä¼ PDFæ–‡ä»¶ï¼ˆæ”¯æŒæ™¶ç››å…‰ä¼/åŒå‘é£ç”µåœºï¼Œå«é˜»å¡/ä»·å·®è´¹ç”¨ï¼‰",
        type=["pdf"],
        accept_multiple_files=True
    )
    
    if uploaded_files and st.button("ğŸš€ å¼€å§‹æ‰¹é‡æå–", type="primary"):
        st.divider()
        st.subheader("âš™ï¸ å¤„ç†è¿›åº¦")
        
        all_results = []
        progress_bar = st.progress(0)
        
        for idx, file in enumerate(uploaded_files):
            st.write(f"æ­£åœ¨å¤„ç†ï¼š{file.name}")
            file_results = parse_pdf_final(file, file.name)
            all_results.extend(file_results)
            progress_bar.progress((idx + 1) / len(uploaded_files))
            file.close()
        
        progress_bar.empty()
        
        df = pd.DataFrame(all_results).fillna("")
        display_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "åŸå§‹ç§‘ç›®ç¼–ç ", "åŸå§‹ç§‘ç›®æ–‡æœ¬", "ç”µé‡(å…†ç“¦æ—¶)", 
            "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_display = df[[col for col in display_cols if col in df.columns]]
        
        st.subheader("ğŸ“ˆ æ‰¹é‡æå–ç»“æœï¼ˆå«é˜»å¡/ä»·å·®è´¹ç”¨ï¼‰")
        def highlight_rows(row):
            if row["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                return ["background-color: #e6f3ff"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Aé£ç”µåœº":
                return ["background-color: #f0fff4"] * len(row)
            elif row["åœºç«™åç§°"] == "åŒå‘Bé£ç”µåœº":
                return ["background-color: #fff8f0"] * len(row)
            elif row["åœºç«™åç§°"] == "æ™¶ç››å…‰ä¼ç”µç«™":
                return ["background-color: #f0f8ff"] * len(row)
            elif "é˜»å¡è´¹ç”¨" in row["ç§‘ç›®åç§°"] or "ä»·å·®è´¹ç”¨" in row["ç§‘ç›®åç§°"]:
                return ["background-color: #ffebee"] * len(row)  # é˜»å¡/ä»·å·®è´¹ç”¨é«˜äº®
            else:
                return [""] * len(row)
        styled_df = df_display.style.apply(highlight_rows, axis=1)
        st.dataframe(styled_df, use_container_width=True)
        
        total_stations = df["åœºç«™åç§°"].nunique()
        total_trades = len(df[(df["ç§‘ç›®åç§°"] != "å½“æ—¥å°è®¡") & (df["ç§‘ç›®åç§°"] != "æ— æœ‰æ•ˆæ•°æ®") & (df["ç§‘ç›®åç§°"] != "è§£æå¤±è´¥")])
        block_spread_count = len(df[(df["ç§‘ç›®åç§°"] == "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨") | (df["ç§‘ç›®åç§°"] == "çœé—´çœå†…ä»·å·®è´¹ç”¨")])
        subtotal_count = len(df[df["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡"])
        st.info(f"**ç»Ÿè®¡ï¼š** è¦†ç›–åœºç«™ {total_stations} ä¸ª | æœ‰æ•ˆç§‘ç›® {total_trades} ä¸ª | é˜»å¡/ä»·å·®è´¹ç”¨ {block_spread_count} ä¸ª | å°è®¡è¡Œ {subtotal_count} ä¸ª")
        
        download_cols = [
            "å…¬å¸åç§°", "åœºç«™åç§°", "æ¸…åˆ†æ—¥æœŸ", "ç§‘ç›®åç§°", 
            "ç”µé‡(å…†ç“¦æ—¶)", "ç”µä»·(å…ƒ/å…†ç“¦æ—¶)", "ç”µè´¹(å…ƒ)"
        ]
        df_download = df[[col for col in download_cols if col in df.columns]]
        
        output = BytesIO()
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_download.to_excel(writer, index=False, sheet_name="å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®")
            ws = writer.sheets["å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®"]
            light_blue = PatternFill(start_color="E6F3FF", end_color="E6F3FF", fill_type="solid")
            light_red = PatternFill(start_color="FFEBEE", end_color="FFEBEE", fill_type="solid")
            for row in range(2, len(df_download) + 2):
                if df_download.iloc[row-2]["ç§‘ç›®åç§°"] == "å½“æ—¥å°è®¡":
                    for col in range(1, len(df_download.columns) + 1):
                        ws.cell(row=row, column=col).fill = light_blue
                elif "é˜»å¡è´¹ç”¨" in df_download.iloc[row-2]["ç§‘ç›®åç§°"] or "ä»·å·®è´¹ç”¨" in df_download.iloc[row-2]["ç§‘ç›®åç§°"]:
                    for col in range(1, len(df_download.columns) + 1):
                        ws.cell(row=row, column=col).fill = light_red
        
        output.seek(0)
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excelï¼ˆå«é˜»å¡/ä»·å·®è´¹ç”¨ï¼‰",
            data=output,
            file_name=f"å¤šåœºç«™æ—¥æ¸…åˆ†æ•°æ®_{datetime.now().strftime('%Y%m%d%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        st.success("âœ… æå–å®Œæˆï¼é˜»å¡/ä»·å·®è´¹ç”¨å·²æ­£å¸¸æå–ï¼Œæ™¶ç››å…‰ä¼ç«™åç§°å·²è¡¥å…¨")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ æ™¶ç››å…‰ä¼ç”µç«™æˆ–åŒå‘A/Bé£ç”µåœºçš„ç°è´§æ—¥æ¸…åˆ†ç»“ç®—å•PDFï¼ˆæ”¯æŒé˜»å¡/ä»·å·®è´¹ç”¨æå–ï¼‰")

if __name__ == "__main__":
    os.environ["PYTHONIOENCODING"] = "utf-8"
    main()
