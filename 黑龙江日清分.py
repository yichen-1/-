import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO
import sys
import os

# å¿½ç•¥æ ·å¼è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆä¿ç•™åŸæœ‰ï¼‰ ----------------------
TRADE_CODE_MAP = {
    "0101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "0101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "0101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "0101040322": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“",
    "0102020101": "é€è¾½å®äº¤æ˜“",
    "0102020301": "é€ååŒ—äº¤æ˜“", 
    "0102010101": "é€å±±ä¸œäº¤æ˜“",
    "0102010201": "é€æµ™æ±Ÿäº¤æ˜“",
    "0202030001": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“",
    "0202030002": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“",
    "0101080101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "0101080201": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "0101080301": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "0101080401": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "0201010101": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "0201020101": "çœé—´çœå†…ä»·å·®è´¹ç”¨",
    "0101050101": "çœå†…ç»¿è‰²ç”µåŠ›äº¤æ˜“(ç”µèƒ½é‡)",
    "0101060101": "æ—¥èåˆäº¤æ˜“",
    "0101070101": "ç°è´§ç»“ç®—ä»·å·®è°ƒæ•´",
    "0101090101": "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š",
    "0101100101": "åå·®è€ƒæ ¸è´¹ç”¨"
}

ALL_TRADES = list(TRADE_CODE_MAP.values())
SPECIAL_TRADES = ["ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨", "çœé—´çœå†…ä»·å·®è´¹ç”¨", "è¾…åŠ©æœåŠ¡è´¹ç”¨åˆ†æ‘Š", "åå·®è€ƒæ ¸è´¹ç”¨"]
REGULAR_TRADES = [trade for trade in ALL_TRADES if trade not in SPECIAL_TRADES]

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•°ï¼ˆå…³é”®ä¿®æ”¹ï¼‰ ----------------------
def safe_convert_to_numeric(value):
    """å®‰å…¨è½¬æ¢ä¸ºæ•°å€¼ - å¢å¼ºç‰ˆï¼Œå…¼å®¹ç½‘é¡µç«¯ç‰¹æ®Šå­—ç¬¦"""
    if value is None or pd.isna(value) or value == '':
        return None
    
    # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²å¤„ç†ï¼Œç§»é™¤ç½‘é¡µç«¯å¸¸è§çš„éæ–­è¡Œç©ºæ ¼(\xa0)
    val_str = str(value).strip().replace('\xa0', ' ')
    
    # æ’é™¤9/10ä½æ•°å­—ï¼ˆç§‘ç›®ç¼–ç ï¼‰
    if re.match(r'^\d{9,10}$', val_str):
        return None
    
    # æ’é™¤ç©ºå­—ç¬¦ä¸²å’Œçº¯ç¬¦å·ï¼ˆè¡¥å……ç½‘é¡µç«¯å¸¸è§ç¬¦å·ï¼‰
    if val_str in ['-', '.', '', 'â€”', 'â€”â€”', ' ', '\t', '\n']:
        return None
    
    try:
        # ç§»é™¤åƒåˆ†ä½é€—å·ã€äººæ°‘å¸ç¬¦å·ã€å…¨è§’ç¬¦å·ç­‰ï¼ˆå¢å¼ºï¼‰
        cleaned = re.sub(r'[^\d.-]', '', val_str.replace('ï¼Œ', ',').replace('ã€‚', '.'))
        if cleaned and cleaned not in ['-', '.', '']:
            return float(cleaned)
        return None
    except (ValueError, TypeError):
        return None

def extract_base_company_info(pdf_text):
    """æå–åŸºç¡€å…¬å¸ä¿¡æ¯ - å¢å¼ºç½‘é¡µç«¯å­—ç¬¦å…¼å®¹"""
    # ç»Ÿä¸€å­—ç¬¦æ ¼å¼ï¼Œç§»é™¤ç‰¹æ®Šç©ºç™½ç¬¦
    pdf_text = pdf_text.replace('\xa0', ' ').replace('\r', '\n').strip()
    lines = pdf_text.split('\n')
    base_company = "æœªçŸ¥å…¬å¸"
    
    for line in lines:
        line_clean = line.strip()
        if "å…¬å¸åç§°" in line_clean:
            # å¢å¼ºæ­£åˆ™ï¼Œå…¼å®¹å…¨è§’å†’å·/ç©ºæ ¼
            match = re.search(r'å…¬å¸åç§°[:ï¼š]\s*(.+?æœ‰é™å…¬å¸)', line_clean)
            if match:
                base_company = match.group(1).strip()
                break
    
    return base_company

def split_double_station_data(pdf_text, pdf_tables):
    """æ‹†åˆ†åŒåœºç«™æ•°æ® - å¢å¼ºé²æ£’æ€§ï¼Œé€‚é…ç½‘é¡µç«¯æ–‡æœ¬"""
    # ç»Ÿä¸€æ–‡æœ¬æ ¼å¼
    pdf_text = pdf_text.replace('\xa0', ' ').replace('\r', '\n').strip()
    base_company = extract_base_company_info(pdf_text)
    lines = pdf_text.split('\n')
    
    station_markers = []
    for i, line in enumerate(lines):
        line_clean = line.strip()
        # å¢å¼ºæ ‡è®°åŒ¹é…ï¼Œå…¼å®¹ç½‘é¡µç«¯å­—ç¬¦å·®å¼‚
        if any(marker in line_clean for marker in ["Aé£ç”µåœº", "Bé£ç”µåœº", "1å·æœºç»„", "2å·æœºç»„", "ä¸€å·æœºç»„", "äºŒå·æœºç»„", "Aåœº", "Båœº"]):
            station_markers.append((i, line_clean))
    
    # ä¼˜åŒ–æ‹†åˆ†é€»è¾‘ï¼šå³ä½¿æ ‡è®°ä¸è¶³2ä¸ªï¼Œä¹Ÿå°è¯•æŒ‰è¡¨æ ¼æ•°é‡æ‹†åˆ†
    if len(station_markers) >= 2:
        station_a_marker = None
        station_b_marker = None
        
        for pos, text in station_markers:
            if any(marker in text for marker in ["Aé£ç”µåœº", "1å·", "ä¸€å·", "Aåœº"]):
                station_a_marker = (pos, f"{base_company}ï¼ˆåŒå‘Aé£ç”µåœºï¼‰")
            elif any(marker in text for marker in ["Bé£ç”µåœº", "2å·", "äºŒå·", "Båœº"]):
                station_b_marker = (pos, f"{base_company}ï¼ˆåŒå‘Bé£ç”µåœºï¼‰")
        
        if station_a_marker and station_b_marker:
            mid_idx = len(pdf_tables) // 2
            station_a_tables = pdf_tables[:mid_idx] if mid_idx > 0 else pdf_tables
            station_b_tables = pdf_tables[mid_idx:] if mid_idx > 0 else []
            # é¿å…ç©ºè¡¨æ ¼
            station_a_tables = station_a_tables if station_a_tables else pdf_tables
            station_b_tables = station_b_tables if station_b_tables else pdf_tables
            
            return [
                (station_a_marker[1], station_a_tables),
                (station_b_marker[1], station_b_tables)
            ]
    
    # å•åœºç«™å¤„ç†ï¼šå¢å¼ºåç§°è¯†åˆ«
    station_name = f"{base_company}ï¼ˆæœªçŸ¥åœºç«™ï¼‰"
    if any(marker in pdf_text for marker in ["Aé£ç”µåœº", "1å·", "ä¸€å·", "Aåœº"]):
        station_name = f"{base_company}ï¼ˆåŒå‘Aé£ç”µåœºï¼‰"
    elif any(marker in pdf_text for marker in ["Bé£ç”µåœº", "2å·", "äºŒå·", "Båœº"]):
        station_name = f"{base_company}ï¼ˆåŒå‘Bé£ç”µåœºï¼‰"
    
    return [(station_name, pdf_tables)]

def extract_station_and_date_v2(pdf_text, file_name, station_name_override=None):
    """æå–åœºç«™åç§°å’Œæ—¥æœŸ - å¢å¼ºç½‘é¡µç«¯æ—¥æœŸåŒ¹é…"""
    # ç»Ÿä¸€æ–‡æœ¬æ ¼å¼
    pdf_text = pdf_text.replace('\xa0', ' ').replace('\r', '\n').strip()
    lines = pdf_text.split('\n')
    
    station_name = station_name_override if station_name_override else "æœªçŸ¥åœºç«™"
    
    if station_name == "æœªçŸ¥åœºç«™":
        for line in lines:
            line_clean = line.strip()
            if "é£ç”µåœº" in line_clean:
                match = re.search(r'([^ï¼Œã€‚ï¼ï¼Ÿã€ï¼›]+é£ç”µåœº)', line_clean)
                if match:
                    station_name = match.group(1).strip()
                    break
    
    # å¢å¼ºæ—¥æœŸåŒ¹é…ï¼šå…¼å®¹æ›´å¤šæ ¼å¼ï¼Œå¤„ç†ç½‘é¡µç«¯å­—ç¬¦
    date = None
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2})',
        r'æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'(\d{4}/\d{1,2}/\d{1,2})',
        r'(\d{4}\.\d{1,2}\.\d{1,2})',
        r'(\d{8})'  # è¡¥å……çº¯æ•°å­—æ—¥æœŸ
    ]
    
    for line in lines:
        line = line.replace('\xa0', ' ')
        for pattern in date_patterns:
            match = re.search(pattern, line)
            if match:
                date_str = match.group(1)
                date_str = date_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('/', '-').replace('.', '-')
                parts = date_str.split('-')
                if len(parts) == 3:
                    year, month, day = parts
                    date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                elif len(date_str) == 8:  # å¤„ç†çº¯æ•°å­—æ—¥æœŸ
                    date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
                break
        if date:
            break
    
    # ä»æ–‡ä»¶åæå–æ—¥æœŸï¼ˆå¢å¼ºï¼šå…¼å®¹æ›´å¤šæ–‡ä»¶åæ ¼å¼ï¼‰
    if not date:
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})|(\d{8})|(\d{4}_\d{2}_\d{2})', file_name)
        if date_match:
            date_str = date_match.group()
            date_str = date_str.replace('_', '-')
            if len(date_str) == 8:
                date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date = date_str
    
    return station_name, date

def extract_data_using_pdfplumber_tables(file_obj):
    """ä½¿ç”¨pdfplumberæå–è¡¨æ ¼ - é€‚é…ç½‘é¡µç«¯pdfplumberç‰ˆæœ¬å·®å¼‚"""
    try:
        # é‡æ–°è¯»å–æ–‡ä»¶æµï¼Œé¿å…ç½‘é¡µç«¯æŒ‡é’ˆåç§»
        file_obj.seek(0)
        with pdfplumber.open(file_obj) as pdf:
            all_tables = []
            for page in pdf.pages:
                # å…¼å®¹ä¸åŒpdfplumberç‰ˆæœ¬çš„å‚æ•°ï¼ˆå…³é”®ä¿®å¤ï¼‰
                try:
                    # æ–°ç‰ˆæœ¬å‚æ•°
                    tables = page.extract_tables({
                        "vertical_strategy": "lines",
                        "horizontal_strategy": "lines",
                        "snap_tolerance": 3,
                        "join_tolerance": 3,
                        "edge_min_length": 10
                    })
                except TypeError:
                    # æ—§ç‰ˆæœ¬å‚æ•°ï¼ˆç½‘é¡µç«¯å¸¸è§ï¼‰
                    tables = page.extract_tables(
                        vertical_strategy="lines",
                        horizontal_strategy="lines",
                        snap_tolerance=3,
                        join_tolerance=3,
                        edge_min_length=10
                    )
                
                if tables:
                    for table in tables:
                        cleaned_table = []
                        for row in table:
                            cleaned_row = []
                            for cell in row:
                                if cell is None:
                                    cleaned_row.append("")
                                else:
                                    # ç§»é™¤ç½‘é¡µç«¯ç‰¹æ®Šå­—ç¬¦
                                    cell_clean = re.sub(r'\s+', ' ', str(cell)).replace('\xa0', ' ').strip()
                                    cleaned_row.append(cell_clean)
                            if any(cell != "" for cell in cleaned_row):
                                cleaned_table.append(cleaned_row)
                        if cleaned_table:
                            all_tables.append(cleaned_table)
            
            return all_tables
    except Exception as e:
        st.error(f"è¡¨æ ¼æå–å¤±è´¥: {e} (pdfplumberç‰ˆæœ¬: {pdfplumber.__version__})")
        return []

def parse_trade_table_data_v2(tables):
    """è§£æäº¤æ˜“è¡¨æ ¼æ•°æ® - å¢å¼ºç½‘é¡µç«¯è¡¨å¤´åŒ¹é…"""
    trade_data = {}
    for trade in ALL_TRADES:
        if trade in SPECIAL_TRADES:
            trade_data[trade] = {'fee': None}
        else:
            trade_data[trade] = {'quantity': None, 'price': None, 'fee': None}
    
    for table in tables:
        if len(table) < 2:  # é™ä½è¡¨å¤´è¡Œæ•°è¦æ±‚ï¼Œå…¼å®¹ç½‘é¡µç«¯è¡¨æ ¼æå–å·®å¼‚
            continue
            
        code_col = -1
        name_col = -1
        qty_col = -1
        price_col = -1
        fee_col = -1
        
        # ä¼˜åŒ–è¡¨å¤´æŸ¥æ‰¾ï¼šå…¼å®¹ç½‘é¡µç«¯è¡¨å¤´è¡Œåç§»
        header_row1 = -1
        for i, row in enumerate(table[:5]):  # åªæ£€æŸ¥å‰5è¡Œï¼Œé¿å…æ— æ•ˆéå†
            row_str = ' '.join([str(cell) for cell in row if cell]).replace('\xa0', ' ')
            if ("ç§‘ç›®ç¼–ç " in row_str or "ç¼–ç " in row_str) and ("ç»“ç®—ç±»å‹" in row_str or "åç§°" in row_str):
                header_row1 = i
                break
        
        if header_row1 == -1:
            # é™çº§åŒ¹é…ï¼šåªæ‰¾ç¼–ç /åç§°åˆ—
            for i, row in enumerate(table[:3]):
                row_str = ' '.join([str(cell) for cell in row if cell]).replace('\xa0', ' ')
                if "ç§‘ç›®ç¼–ç " in row_str or "ç¼–ç " in row_str:
                    header_row1 = i
                    break
        
        if header_row1 == -1:
            continue
        
        header_row2 = header_row1 + 1
        if header_row2 >= len(table):
            header_row2 = header_row1  # å…¼å®¹å•è¡Œè¡¨å¤´
        
        # åŒ¹é…åˆ—ç´¢å¼•ï¼šå¢å¼ºå®¹é”™
        for j, cell in enumerate(table[header_row1]):
            cell_lower = str(cell).lower().replace('\xa0', ' ')
            if any(keyword in cell_lower for keyword in ["ç§‘ç›®ç¼–ç ", "ç¼–ç ", "code"]):
                code_col = j
            elif any(keyword in cell_lower for keyword in ["ç»“ç®—ç±»å‹", "ç§‘ç›®åç§°", "åç§°", "name"]):
                name_col = j
        
        for j, cell in enumerate(table[header_row2]):
            cell_lower = str(cell).lower().replace('\xa0', ' ')
            if any(keyword in cell_lower for keyword in ["ç”µé‡", "æ•°é‡", "kwh", "mwh", "å…†ç“¦æ—¶"]):
                qty_col = j
            elif any(keyword in cell_lower for keyword in ["ç”µä»·", "ä»·æ ¼", "å•ä»·", "price"]):
                price_col = j
            elif any(keyword in cell_lower for keyword in ["ç”µè´¹", "é‡‘é¢", "è´¹ç”¨", "åˆè®¡", "amount", "å…ƒ"]):
                fee_col = j
        
        # è§£ææ•°æ®è¡Œï¼šå¢å¼ºå®¹é”™
        for i in range(header_row2 + 1, len(table)):
            row = table[i]
            row_str = ' '.join([str(cell) for cell in row if cell]).replace('\xa0', ' ')
            if any(keyword in row_str for keyword in ["åˆè®¡", "æ€»è®¡", "å°è®¡", "summary", "total", "æ±‡æ€»"]):
                continue
            
            trade_code = ""
            trade_name = None
            
            # ç¼–ç åˆ—æå–ï¼šå…¼å®¹ç½‘é¡µç«¯ç¼–ç æ ¼å¼
            if code_col >= 0 and code_col < len(row):
                trade_code = str(row[code_col]).strip().replace('\xa0', ' ')
                if len(trade_code) == 9:
                    trade_code = "0" + trade_code
                if trade_code in TRADE_CODE_MAP:
                    trade_name = TRADE_CODE_MAP[trade_code]
            
            # åç§°åˆ—æ¨¡ç³ŠåŒ¹é…ï¼šå¢å¼ºå…³é”®è¯åŒ¹é…
            if not trade_name and name_col >= 0 and name_col < len(row):
                name_cell = str(row[name_col]).strip().replace('\xa0', ' ')
                # æ‹†åˆ†å…³é”®è¯ï¼Œå¢å¼ºåŒ¹é…
                for code, name in TRADE_CODE_MAP.items():
                    name_parts = re.split(r'[()ï¼ˆï¼‰ã€-]', name)
                    if any(part.strip() in name_cell for part in name_parts if part.strip()):
                        trade_name = name
                        break
            
            if not trade_name:
                continue
            
            is_special = trade_name in SPECIAL_TRADES
            if is_special:
                if fee_col >= 0 and fee_col < len(row):
                    fee_val = row[fee_col]
                    trade_data[trade_name]['fee'] = safe_convert_to_numeric(fee_val)
            else:
                if qty_col >= 0 and qty_col < len(row):
                    qty_val = row[qty_col]
                    trade_data[trade_name]['quantity'] = safe_convert_to_numeric(qty_val)
                
                if price_col >= 0 and price_col < len(row):
                    price_val = row[price_col]
                    trade_data[trade_name]['price'] = safe_convert_to_numeric(price_val)
                
                if fee_col >= 0 and fee_col < len(row):
                    fee_val = row[fee_col]
                    trade_data[trade_name]['fee'] = safe_convert_to_numeric(fee_val)
    
    return trade_data

def extract_total_data_v2(pdf_text):
    """æå–åˆè®¡æ•°æ® - å¢å¼ºç½‘é¡µç«¯æ–‡æœ¬åŒ¹é…"""
    pdf_text = pdf_text.replace('\xa0', ' ').replace('\r', '\n').strip()
    total_quantity, total_amount = None, None
    lines = pdf_text.split('\n')
    
    for line in lines:
        line_clean = line.replace(' ', '').replace(',', '').replace('ï¼Œ', '').replace('\xa0', '')
        # å¢å¼ºåˆè®¡åŒ¹é…ï¼šå…¼å®¹æ›´å¤šè¡¨è¿°
        qty_match = re.search(r'åˆè®¡ç”µé‡[:ï¼š]([\d\.]+)|æ€»ç”µé‡[:ï¼š]([\d\.]+)|ç”µé‡åˆè®¡[:ï¼š]([\d\.]+)', line_clean)
        if qty_match:
            # å–ç¬¬ä¸€ä¸ªéç©ºåŒ¹é…ç»„
            qty_val = next((g for g in qty_match.groups() if g), None)
            if qty_val:
                total_quantity = safe_convert_to_numeric(qty_val)
        
        fee_match = re.search(r'åˆè®¡ç”µè´¹[:ï¼š]([\d\.]+)|æ€»ç”µè´¹[:ï¼š]([\d\.]+)|ç”µè´¹åˆè®¡[:ï¼š]([\d\.]+)|åˆè®¡é‡‘é¢[:ï¼š]([\d\.]+)', line_clean)
        if fee_match:
            fee_val = next((g for g in fee_match.groups() if g), None)
            if fee_val:
                total_amount = safe_convert_to_numeric(fee_val)
    
    return total_quantity, total_amount

def process_single_station(station_name, tables, pdf_text, file_name):
    """å¤„ç†å•ä¸ªåœºç«™ - ä¿ç•™åŸæœ‰é€»è¾‘"""
    station_name, date = extract_station_and_date_v2(pdf_text, file_name, station_name)
    total_quantity, total_amount = extract_total_data_v2(pdf_text)
    trade_data = parse_trade_table_data_v2(tables)
    
    result = [station_name, date, total_quantity, total_amount]
    for trade in REGULAR_TRADES:
        data = trade_data.get(trade, {'quantity': None, 'price': None, 'fee': None})
        result.extend([data['quantity'], data['price'], data['fee']])
    for trade in SPECIAL_TRADES:
        data = trade_data.get(trade, {'fee': None})
        result.append(data['fee'])
    
    return result

def extract_data_from_pdf_v2(file_obj, file_name):
    """ä»PDFæå–æ•°æ® - å…³é”®ä¿®å¤ï¼šå½»åº•é‡ç½®æ–‡ä»¶æµ"""
    try:
        # å…³é”®ä¿®æ”¹1ï¼šå¤åˆ¶æ–‡ä»¶æµåˆ°BytesIOï¼Œé¿å…ç½‘é¡µç«¯æ–‡ä»¶å¯¹è±¡é™åˆ¶
        file_obj.seek(0)
        file_bytes = BytesIO(file_obj.read())
        file_bytes.seek(0)
        
        # è¯»å–PDFæ–‡æœ¬
        with pdfplumber.open(file_bytes) as pdf:
            all_text = ""
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    all_text += text + "\n"
        
        if not all_text or len(all_text.strip()) < 50:
            raise ValueError("PDFä¸ºç©ºæˆ–æ–‡æœ¬å†…å®¹å¤ªå°‘")
        
        # é‡æ–°é‡ç½®å­—èŠ‚æµï¼Œæå–è¡¨æ ¼
        file_bytes.seek(0)
        all_tables = extract_data_using_pdfplumber_tables(file_bytes)
        
        if not all_tables:
            st.warning(f"{file_name}: è¡¨æ ¼æå–å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åˆ†ææ¨¡å¼")
        
        # æ‹†åˆ†åŒåœºç«™æ•°æ®
        station_data_list = split_double_station_data(all_text, all_tables)
        
        # å¤„ç†æ¯ä¸ªåœºç«™
        results = []
        for station_name, tables_segment in station_data_list:
            result = process_single_station(station_name, tables_segment, all_text, file_name)
            results.append(result)
        
        # å…³é—­å­—èŠ‚æµï¼Œé‡Šæ”¾èµ„æº
        file_bytes.close()
        return results
        
    except Exception as e:
        st.error(f"å¤„ç†PDF {file_name} å‡ºé”™: {str(e)}")
        default_result = ["æœªçŸ¥åœºç«™", None, None, None] + [None] * (len(REGULAR_TRADES) * 3 + len(SPECIAL_TRADES))
        return [default_result]

# ---------------------- Streamlit åº”ç”¨ï¼ˆå…³é”®ä¿®æ”¹ï¼‰ ----------------------
def main():
    st.set_page_config(page_title="é»‘é¾™æ±Ÿæ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·", layout="wide")
    
    st.title("ğŸ“Š é»‘é¾™æ±Ÿæ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆç½‘é¡µé€‚é…ç‰ˆï¼‰")
    st.markdown("**æ ¸å¿ƒä¿®å¤ï¼šé€‚é…ç½‘é¡µç«¯è¿è¡Œç¯å¢ƒã€ç»Ÿä¸€æ–‡ä»¶æµå¤„ç†ã€å¢å¼ºå­—ç¬¦å…¼å®¹**")
    st.divider()
    
    # æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
    with st.expander("ğŸ”§ è¿è¡Œç¯å¢ƒä¿¡æ¯ï¼ˆè°ƒè¯•ï¼‰"):
        st.write(f"Pythonç‰ˆæœ¬: {sys.version}")
        st.write(f"pdfplumberç‰ˆæœ¬: {pdfplumber.__version__}")
        st.write(f"pandasç‰ˆæœ¬: {pd.__version__}")
    
    # æ˜¾ç¤ºç§‘ç›®ä¿¡æ¯
    with st.expander("ğŸ“‹ æ”¯æŒçš„ç§‘ç›®åˆ—è¡¨ï¼ˆå«æ–°å¢ï¼‰"):
        st.write("**å¸¸è§„ç§‘ç›®ï¼ˆç”µé‡ã€ç”µä»·ã€ç”µè´¹ï¼‰ï¼š**")
        for trade in REGULAR_TRADES:
            st.write(f"- {trade}")
        st.write("**ç‰¹æ®Šç§‘ç›®ï¼ˆä»…ç”µè´¹ï¼‰ï¼š**")
        for trade in SPECIAL_TRADES:
            st.write(f"- {trade}")
    
    st.subheader("ğŸ“ ä¸Šä¼ æ–‡ä»¶")
    uploaded_files = st.file_uploader(
        "æ”¯æŒPDFæ ¼å¼ï¼Œå¯æ‰¹é‡ä¸Šä¼ ï¼ˆé€‚é…ä¾å…°ååˆé£ç”µPDFï¼‰",
        type=['pdf'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary"):
            st.divider()
            st.subheader("âš™ï¸ å¤„ç†è¿›åº¦")
            
            all_data = []
            progress_bar = st.progress(0)
            
            for idx, file in enumerate(uploaded_files):
                progress_bar.progress((idx + 1) / len(uploaded_files))
                
                try:
                    # å…³é”®ä¿®æ”¹2ï¼šæ‰¹é‡å¤„ç†æ—¶æ¯æ¬¡é‡ç½®æ–‡ä»¶æµ
                    file.seek(0)
                    file_results = extract_data_from_pdf_v2(file, file.name)
                    for result in file_results:
                        all_data.append(result)
                    
                    if len(file_results) == 2:
                        st.success(f"âœ“ {file.name} å¤„ç†æˆåŠŸï¼ˆè¯†åˆ«å‡º2ä¸ªåœºç«™ï¼‰")
                    elif len(file_results) == 1:
                        st.success(f"âœ“ {file.name} å¤„ç†æˆåŠŸï¼ˆè¯†åˆ«å‡º1ä¸ªåœºç«™ï¼‰")
                    else:
                        st.warning(f"âš  {file.name} å¤„ç†å®Œæˆï¼Œä½†æœªè¯†åˆ«åˆ°åœºç«™æ•°æ®")
                        
                except Exception as e:
                    st.error(f"âœ— {file.name} å¤„ç†å¤±è´¥: {str(e)}")
                finally:
                    # å…³é”®ä¿®æ”¹3ï¼šé‡Šæ”¾æ–‡ä»¶èµ„æº
                    file.close()
            
            progress_bar.empty()
            
            if all_data:
                # æ„å»ºç»“æœDataFrame
                result_columns = ['åœºç«™åç§°', 'æ¸…åˆ†æ—¥æœŸ', 'åˆè®¡ç”µé‡(å…†ç“¦æ—¶)', 'åˆè®¡ç”µè´¹(å…ƒ)']
                for trade in REGULAR_TRADES:
                    trade_short = trade.replace('çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“', 'çœé—´ç»¿ç”µäº¤æ˜“')
                    trade_short = trade_short.replace('ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“', 'ä»£ç†è´­ç”µäº¤æ˜“')
                    trade_short = trade_short.replace('(ç”µèƒ½é‡)', '')
                    result_columns.extend([f'{trade_short}_ç”µé‡', f'{trade_short}_ç”µä»·', f'{trade_short}_ç”µè´¹'])
                for trade in SPECIAL_TRADES:
                    result_columns.append(f'{trade}_ç”µè´¹')
                
                result_df = pd.DataFrame(all_data, columns=result_columns)
                
                # æ˜¾ç¤ºç»“æœ
                st.subheader("ğŸ“ˆ æå–ç»“æœ")
                st.dataframe(result_df, use_container_width=True)
                
                # å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
                st.info(f"**ç»Ÿè®¡ä¿¡æ¯ï¼š** å…±å¤„ç† {len(all_data)} æ¡åœºç«™è®°å½•ï¼Œæ¶‰åŠ {result_df['åœºç«™åç§°'].nunique()} ä¸ªåœºç«™")
                
                # æ£€æŸ¥åŒå‘A/Bé£ç”µåœº
                has_a_station = any('åŒå‘A' in str(name) for name in result_df['åœºç«™åç§°'])
                has_b_station = any('åŒå‘B' in str(name) for name in result_df['åœºç«™åç§°'])
                
                if has_a_station and has_b_station:
                    st.success("âœ… æˆåŠŸè¯†åˆ«åŒå‘A/Bé£ç”µåœºæ•°æ®")
                elif has_a_station:
                    st.warning("âš ï¸ ä»…è¯†åˆ«åˆ°åŒå‘Aé£ç”µåœºï¼Œæœªæ£€æµ‹åˆ°Bé£ç”µåœºæ•°æ®")
                elif has_b_station:
                    st.warning("âš ï¸ ä»…è¯†åˆ«åˆ°åŒå‘Bé£ç”µåœºï¼Œæœªæ£€æµ‹åˆ°Aé£ç”µåœºæ•°æ®")
                
                # æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
                data_columns = result_columns[4:]
                non_null_count = result_df[data_columns].notna().sum()
                total_cells = len(result_df) * len(data_columns)
                filled_cells = result_df[data_columns].notna().sum().sum()
                
                st.info(f"**æ•°æ®å®Œæ•´æ€§ï¼š**")
                st.info(f"- æ€»æ•°æ®å•å…ƒæ ¼ï¼š{total_cells}")
                st.info(f"- æœ‰å€¼å•å…ƒæ ¼ï¼š{filled_cells} ({filled_cells/total_cells*100:.1f}%)")
                
                # ä¸‹è½½åŠŸèƒ½
                current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    result_df.to_excel(writer, index=False)
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
                    data=output,
                    file_name=f"é»‘é¾™æ±Ÿç»“ç®—æ•°æ®_ç½‘é¡µé€‚é…ç‰ˆ_{current_time}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    type="primary"
                )
                
                st.success("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ PDFæ–‡ä»¶å¼€å§‹å¤„ç†ï¼ˆå·²é€‚é…ä¾å…°ååˆé£ç”µPDFï¼‰")

if __name__ == "__main__":
    # å…³é”®ä¿®æ”¹4ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…Streamlitç½‘é¡µç«¯ç¼–ç é—®é¢˜
    os.environ["PYTHONIOENCODING"] = "utf-8"
    main()
