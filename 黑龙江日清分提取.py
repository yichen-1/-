import streamlit as st
import pandas as pd
import re
from datetime import datetime
import warnings
import pdfplumber
from io import BytesIO

# å¿½ç•¥æ ·å¼è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl.stylesheet")

# ---------------------- æ ¸å¿ƒé…ç½® ----------------------
# ç§‘ç›®ç¼–ç åˆ°åç§°çš„å®Œæ•´æ˜ å°„
TRADE_CODE_MAP = {
    "101010101": "ä¼˜å…ˆå‘ç”µäº¤æ˜“",
    "101020101": "ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“", 
    "101020301": "çœå†…ç”µåŠ›ç›´æ¥äº¤æ˜“",
    "101040322": "é€ä¸Šæµ·çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“",
    "102020101": "é€è¾½å®äº¤æ˜“",
    "102020301": "é€ååŒ—äº¤æ˜“", 
    "102010101": "é€å±±ä¸œäº¤æ˜“",
    "102010201": "é€æµ™æ±Ÿäº¤æ˜“",
    "202030001": "é€æ±Ÿè‹çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“",
    "202030002": "é€æµ™æ±Ÿçœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“",
    "101080101": "çœå†…ç°è´§æ—¥å‰äº¤æ˜“",
    "101080201": "çœå†…ç°è´§å®æ—¶äº¤æ˜“",
    "101080301": "çœé—´ç°è´§æ—¥å‰äº¤æ˜“",
    "101080401": "çœé—´ç°è´§æ—¥å†…äº¤æ˜“",
    "201010101": "ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨",
    "201020101": "çœé—´çœå†…ä»·å·®è´¹ç”¨"
}

ALL_TRADES = list(TRADE_CODE_MAP.values())
SPECIAL_TRADES = ["ä¸­é•¿æœŸåˆçº¦é˜»å¡è´¹ç”¨", "çœé—´çœå†…ä»·å·®è´¹ç”¨"]
REGULAR_TRADES = [trade for trade in ALL_TRADES if trade not in SPECIAL_TRADES]

# ---------------------- æ ¸å¿ƒå·¥å…·å‡½æ•° ----------------------
def safe_convert_to_numeric(value):
    """å®‰å…¨è½¬æ¢ä¸ºæ•°å€¼ - å¢å¼ºç‰ˆï¼Œé¿å…å°†ç¼–ç è¯†åˆ«ä¸ºæ•°å­—"""
    if value is None or pd.isna(value) or value == '':
        return None
    
    # å…ˆè½¬ä¸ºå­—ç¬¦ä¸²å¤„ç†
    val_str = str(value).strip()
    
    # æ’é™¤9ä½æ•°å­—ï¼ˆç§‘ç›®ç¼–ç ï¼‰
    if re.match(r'^\d{9}$', val_str):
        return None
    
    # æ’é™¤ç©ºå­—ç¬¦ä¸²å’Œçº¯ç¬¦å·
    if val_str in ['-', '.', '', 'â€”', 'â€”â€”']:
        return None
    
    try:
        # ç§»é™¤åƒåˆ†ä½é€—å·ã€äººæ°‘å¸ç¬¦å·ç­‰
        cleaned = re.sub(r'[^\d.-]', '', val_str)
        if cleaned and cleaned not in ['-', '.', '']:
            return float(cleaned)
        return None
    except (ValueError, TypeError):
        return None

def extract_base_company_info(pdf_text):
    """æå–åŸºç¡€å…¬å¸ä¿¡æ¯ï¼ˆç”¨äºåŒåœºç«™è¯†åˆ«ï¼‰"""
    lines = pdf_text.split('\n')
    base_company = "æœªçŸ¥å…¬å¸"
    
    # æå–åŸºç¡€å…¬å¸åç§°
    for line in lines:
        line_clean = line.strip()
        if "å…¬å¸åç§°" in line_clean:
            match = re.search(r'å…¬å¸åç§°[:ï¼š]\s*(.+?æœ‰é™å…¬å¸)', line_clean)
            if match:
                base_company = match.group(1).strip()
                break
    
    return base_company

def split_double_station_data(pdf_text, pdf_tables):
    """
    æ‹†åˆ†åŒåœºç«™ï¼ˆA/Bï¼‰æ•°æ®
    è¿”å›ï¼š[(station_name, tables_segment), ...]
    """
    base_company = extract_base_company_info(pdf_text)
    lines = pdf_text.split('\n')
    
    # æ ‡è®°ç‚¹ï¼šæŸ¥æ‰¾åŒ…å«"Aé£ç”µåœº"ã€"Bé£ç”µåœº"ã€"1å·"ã€"2å·"ã€"ä¸€å·"ã€"äºŒå·"çš„è¡Œ
    station_markers = []
    for i, line in enumerate(lines):
        line_clean = line.strip()
        if any(marker in line_clean for marker in ["Aé£ç”µåœº", "Bé£ç”µåœº", "1å·æœºç»„", "2å·æœºç»„", "ä¸€å·æœºç»„", "äºŒå·æœºç»„"]):
            station_markers.append((i, line_clean))
    
    # æƒ…å†µ1ï¼šæ£€æµ‹åˆ°åŒåœºç«™æ ‡è®°
    if len(station_markers) >= 2:
        # è¯†åˆ«A/Båœºç«™
        station_a_marker = None
        station_b_marker = None
        
        for pos, text in station_markers:
            if any(marker in text for marker in ["Aé£ç”µåœº", "1å·", "ä¸€å·"]):
                station_a_marker = (pos, f"{base_company}ï¼ˆåŒå‘Aé£ç”µåœºï¼‰")
            elif any(marker in text for marker in ["Bé£ç”µåœº", "2å·", "äºŒå·"]):
                station_b_marker = (pos, f"{base_company}ï¼ˆåŒå‘Bé£ç”µåœºï¼‰")
        
        # å¦‚æœæ‰¾åˆ°A/Bæ ‡è®°ï¼Œå°è¯•æ‹†åˆ†è¡¨æ ¼
        if station_a_marker and station_b_marker:
            # ç®€å•ç­–ç•¥ï¼šå°†è¡¨æ ¼åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼ˆå®é™…å¯æ ¹æ®PDFç»“æ„ä¼˜åŒ–ï¼‰
            mid_idx = len(pdf_tables) // 2
            station_a_tables = pdf_tables[:mid_idx]
            station_b_tables = pdf_tables[mid_idx:]
            
            return [
                (station_a_marker[1], station_a_tables),
                (station_b_marker[1], station_b_tables)
            ]
    
    # æƒ…å†µ2ï¼šå•åœºç«™æˆ–æ— æ³•æ‹†åˆ†ï¼Œè¿”å›æ•´ä½“
    # å°è¯•è¯†åˆ«æ˜¯Aè¿˜æ˜¯B
    station_name = f"{base_company}ï¼ˆæœªçŸ¥åœºç«™ï¼‰"
    if any(marker in pdf_text for marker in ["Aé£ç”µåœº", "1å·", "ä¸€å·"]):
        station_name = f"{base_company}ï¼ˆåŒå‘Aé£ç”µåœºï¼‰"
    elif any(marker in pdf_text for marker in ["Bé£ç”µåœº", "2å·", "äºŒå·"]):
        station_name = f"{base_company}ï¼ˆåŒå‘Bé£ç”µåœºï¼‰"
    
    return [(station_name, pdf_tables)]

def extract_station_and_date_v2(pdf_text, file_name, station_name_override=None):
    """æå–åœºç«™åç§°å’Œæ—¥æœŸ - å¢å¼ºç‰ˆï¼Œæ”¯æŒåœºç«™åç§°è¦†ç›–"""
    lines = pdf_text.split('\n')
    
    # ä½¿ç”¨è¦†ç›–çš„åœºç«™åç§°ï¼ˆåŒåœºç«™æ‹†åˆ†æ—¶ç”¨ï¼‰
    station_name = station_name_override if station_name_override else "æœªçŸ¥åœºç«™"
    
    # å¦‚æœæ²¡æœ‰è¦†ç›–åç§°ï¼Œå°è¯•ä»æ–‡æœ¬æå–
    if station_name == "æœªçŸ¥åœºç«™":
        # æ–¹æ³•1: ä»åŒ…å«é£ç”µåœºçš„è¡Œæå–
        for line in lines:
            line_clean = line.strip()
            if "é£ç”µåœº" in line_clean:
                match = re.search(r'([^ï¼Œã€‚ï¼ï¼Ÿ]+é£ç”µåœº)', line_clean)
                if match:
                    station_name = match.group(1).strip()
                    break
    
    # æå–æ—¥æœŸ - å¢å¼ºçš„åŒ¹é…æ¨¡å¼
    date = None
    date_patterns = [
        r'æ¸…åˆ†æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2})',
        r'æ—¥æœŸ[:ï¼š]\s*(\d{4}-\d{1,2}-\d{1,2})',
        r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)',
        r'(\d{4}/\d{1,2}/\d{1,2})',
        r'(\d{4}\.\d{1,2}\.\d{1,2})'
    ]
    
    for line in lines:
        for pattern in date_patterns:
            match = re.search(pattern, line)
            if match:
                date_str = match.group(1)
                # ç»Ÿä¸€è½¬æ¢ä¸ºyyyy-mm-ddæ ¼å¼
                date_str = date_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('/', '-').replace('.', '-')
                # è¡¥å…¨æœˆä»½å’Œæ—¥æœŸçš„å‰å¯¼é›¶
                parts = date_str.split('-')
                if len(parts) == 3:
                    year, month, day = parts
                    date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                break
        if date:
            break
    
    # ä»æ–‡ä»¶åæå–æ—¥æœŸï¼ˆå¤‡ç”¨ï¼‰
    if not date:
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})|(\d{8})', file_name)
        if date_match:
            date_str = date_match.group()
            if len(date_str) == 8:  # yyyymmddæ ¼å¼
                date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:]}"
            else:
                date = date_str
    
    return station_name, date

def extract_data_using_pdfplumber_tables(file_obj):
    """ä½¿ç”¨pdfplumberçš„è¡¨æ ¼æå–åŠŸèƒ½ - å¢å¼ºç‰ˆ"""
    try:
        with pdfplumber.open(file_obj) as pdf:
            all_tables = []
            for page in pdf.pages:
                # ä¼˜åŒ–è¡¨æ ¼æå–å‚æ•°
                tables = page.extract_tables({
                    "vertical_strategy": "lines",
                    "horizontal_strategy": "lines",
                    "snap_tolerance": 3,
                    "join_tolerance": 3
                })
                if tables:
                    for table in tables:
                        # æ·±åº¦æ¸…ç†è¡¨æ ¼æ•°æ®
                        cleaned_table = []
                        for row in table:
                            cleaned_row = []
                            for cell in row:
                                if cell is None:
                                    cleaned_row.append("")
                                else:
                                    # ç§»é™¤ç©ºç™½å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·
                                    cell_clean = re.sub(r'\s+', ' ', str(cell)).strip()
                                    cleaned_row.append(cell_clean)
                            # è·³è¿‡ç©ºè¡Œ
                            if any(cell != "" for cell in cleaned_row):
                                cleaned_table.append(cleaned_row)
                        if cleaned_table:  # åªæ·»åŠ éç©ºè¡¨æ ¼
                            all_tables.append(cleaned_table)
            
            return all_tables
    except Exception as e:
        st.error(f"è¡¨æ ¼æå–å¤±è´¥: {e}")
        return []

def parse_trade_table_data_v2(tables):
    """è§£æäº¤æ˜“è¡¨æ ¼æ•°æ® - å¢å¼ºç‰ˆï¼Œé¿å…ç¼–ç è¯†åˆ«é”™è¯¯"""
    trade_data = {}
    
    # åˆå§‹åŒ–æ‰€æœ‰ç§‘ç›®
    for trade in ALL_TRADES:
        if trade in SPECIAL_TRADES:
            trade_data[trade] = {'fee': None}
        else:
            trade_data[trade] = {'quantity': None, 'price': None, 'fee': None}
    
    for table in tables:
        if len(table) < 2:  # è‡³å°‘è¦æœ‰è¡¨å¤´å’Œæ•°æ®è¡Œ
            continue
            
        # æ™ºèƒ½æŸ¥æ‰¾è¡¨å¤´è¡Œ
        header_row = -1
        code_col = -1
        name_col = -1
        qty_col = -1
        price_col = -1
        fee_col = -1
        
        # éå†æ‰€æœ‰è¡Œå¯»æ‰¾è¡¨å¤´
        for i, row in enumerate(table):
            row_str = ' '.join([str(cell) for cell in row if cell])
            # æ›´å®½æ¾çš„è¡¨å¤´è¯†åˆ«
            if ("ç§‘ç›®ç¼–ç " in row_str or "ç¼–ç " in row_str) and ("ç§‘ç›®åç§°" in row_str or "åç§°" in row_str):
                header_row = i
                # ç¡®å®šå„åˆ—ä½ç½®ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰
                for j, cell in enumerate(row):
                    cell_lower = str(cell).lower()
                    if any(keyword in cell_lower for keyword in ["ç§‘ç›®ç¼–ç ", "ç¼–ç ", "code"]):
                        code_col = j
                    elif any(keyword in cell_lower for keyword in ["ç§‘ç›®åç§°", "åç§°", "name"]):
                        name_col = j
                    elif any(keyword in cell_lower for keyword in ["ç”µé‡", "æ•°é‡", "kwh", "mwh"]):
                        qty_col = j
                    elif any(keyword in cell_lower for keyword in ["ç”µä»·", "ä»·æ ¼", "price"]):
                        price_col = j
                    elif any(keyword in cell_lower for keyword in ["ç”µè´¹", "é‡‘é¢", "è´¹ç”¨", "amount"]):
                        fee_col = j
                break
        
        if header_row == -1:
            continue
            
        # è§£ææ•°æ®è¡Œï¼ˆè·³è¿‡è¡¨å¤´å’Œåˆè®¡è¡Œï¼‰
        for i in range(header_row + 1, len(table)):
            row = table[i]
            # è·³è¿‡åˆè®¡/æ€»è®¡è¡Œ
            row_str = ' '.join([str(cell) for cell in row if cell])
            if any(keyword in row_str for keyword in ["åˆè®¡", "æ€»è®¡", "å°è®¡", "summary", "total"]):
                continue
            
            # æå–ç§‘ç›®ç¼–ç å’Œåç§°
            trade_code = ""
            trade_name = None
            
            # ä»ç¼–ç åˆ—æå–
            if code_col >= 0 and code_col < len(row):
                trade_code = str(row[code_col]).strip()
                if trade_code in TRADE_CODE_MAP:
                    trade_name = TRADE_CODE_MAP[trade_code]
            
            # ç¼–ç åŒ¹é…å¤±è´¥ï¼Œå°è¯•ä»åç§°åˆ—åŒ¹é…
            if not trade_name and name_col >= 0 and name_col < len(row):
                name_cell = str(row[name_col]).strip()
                for code, name in TRADE_CODE_MAP.items():
                    if name in name_cell or name.replace("äº¤æ˜“", "") in name_cell:
                        trade_name = name
                        break
            
            if not trade_name:
                continue
            
            # æå–æ•°æ®ï¼ˆä½¿ç”¨å®‰å…¨è½¬æ¢å‡½æ•°ï¼‰
            is_special = trade_name in SPECIAL_TRADES
            
            if is_special:
                # ç‰¹æ®Šç§‘ç›®åªæœ‰ç”µè´¹
                if fee_col >= 0 and fee_col < len(row):
                    fee_val = row[fee_col]
                    trade_data[trade_name]['fee'] = safe_convert_to_numeric(fee_val)
            else:
                # å¸¸è§„ç§‘ç›® - æ›´å®¹é”™çš„æå–é€»è¾‘
                if qty_col >= 0 and qty_col < len(row):
                    qty_val = row[qty_col]
                    trade_data[trade_name]['quantity'] = safe_convert_to_numeric(qty_val)
                
                if price_col >= 0 and price_col < len(row):
                    price_val = row[price_col]
                    trade_data[trade_name]['price'] = safe_convert_to_numeric(price_val)
                
                if fee_col >= 0 and fee_col < len(row):
                    fee_val = row[fee_col]
                    trade_data[trade_name]['fee'] = safe_convert_to_numeric(fee_val)
    
    return trade_data

def extract_total_data_v2(pdf_text):
    """æå–åˆè®¡æ•°æ® - å¢å¼ºç‰ˆ"""
    total_quantity, total_amount = None, None
    
    lines = pdf_text.split('\n')
    
    for line in lines:
        line_clean = line.replace(' ', '').replace(',', '').replace('ï¼Œ', '')
        
        # æ›´ç²¾å‡†çš„åˆè®¡ç”µé‡æå–
        qty_match = re.search(r'åˆè®¡ç”µé‡[:ï¼š]([\d\.]+)', line_clean)
        if qty_match:
            total_quantity = safe_convert_to_numeric(qty_match.group(1))
        
        # æ›´ç²¾å‡†çš„åˆè®¡ç”µè´¹æå–
        fee_match = re.search(r'åˆè®¡ç”µè´¹[:ï¼š]([\d\.]+)', line_clean)
        if fee_match:
            total_amount = safe_convert_to_numeric(fee_match.group(1))
    
    return total_quantity, total_amount

def process_single_station(station_name, tables, pdf_text, file_name):
    """å¤„ç†å•ä¸ªåœºç«™çš„æ•°æ®æå–"""
    # æå–åŸºç¡€ä¿¡æ¯
    station_name, date = extract_station_and_date_v2(pdf_text, file_name, station_name)
    total_quantity, total_amount = extract_total_data_v2(pdf_text)
    
    # è§£æäº¤æ˜“æ•°æ®
    trade_data = parse_trade_table_data_v2(tables)
    
    # æ„å»ºç»“æœåˆ—è¡¨
    result = [station_name, date, total_quantity, total_amount]
    
    # æ·»åŠ å¸¸è§„ç§‘ç›®æ•°æ®
    for trade in REGULAR_TRADES:
        data = trade_data.get(trade, {'quantity': None, 'price': None, 'fee': None})
        result.extend([data['quantity'], data['price'], data['fee']])
    
    # æ·»åŠ ç‰¹æ®Šç§‘ç›®æ•°æ®
    for trade in SPECIAL_TRADES:
        data = trade_data.get(trade, {'fee': None})
        result.append(data['fee'])
    
    return result

def extract_data_from_pdf_v2(file_obj, file_name):
    """ä»PDFæå–æ•°æ® - æ”¯æŒåŒåœºç«™ç‰ˆæœ¬"""
    try:
        # é¦–å…ˆè¯»å–PDFæ–‡æœ¬å’Œè¡¨æ ¼
        file_obj.seek(0)
        with pdfplumber.open(file_obj) as pdf:
            all_text = ""
            for page in pdf.pages:
                text = page.extract_text()
                if text:
                    all_text += text + "\n"
        
        if not all_text or len(all_text.strip()) < 50:
            raise ValueError("PDFä¸ºç©ºæˆ–æ–‡æœ¬å†…å®¹å¤ªå°‘")
        
        # æå–è¡¨æ ¼æ•°æ®
        file_obj.seek(0)
        all_tables = extract_data_using_pdfplumber_tables(file_obj)
        
        if not all_tables:
            # è¡¨æ ¼æå–å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åˆ†æï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            st.warning(f"{file_name}: è¡¨æ ¼æå–å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬åˆ†ææ¨¡å¼")
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ–‡æœ¬åˆ†æçš„å¤‡ç”¨é€»è¾‘
        
        # æ‹†åˆ†åŒåœºç«™æ•°æ®
        station_data_list = split_double_station_data(all_text, all_tables)
        
        # å¤„ç†æ¯ä¸ªåœºç«™
        results = []
        for station_name, tables_segment in station_data_list:
            result = process_single_station(station_name, tables_segment, all_text, file_name)
            results.append(result)
        
        return results
        
    except Exception as e:
        st.error(f"å¤„ç†PDF {file_name} å‡ºé”™: {str(e)}")
        # è¿”å›é»˜è®¤æ ¼å¼çš„é”™è¯¯æ•°æ®
        default_result = ["æœªçŸ¥åœºç«™", None, None, None] + [None] * (len(REGULAR_TRADES) * 3 + len(SPECIAL_TRADES))
        return [default_result]

# ---------------------- Streamlit åº”ç”¨ ----------------------
def main():
    st.set_page_config(page_title="é»‘é¾™æ±Ÿæ—¥æ¸…åˆ†æ•°æ®æå–å·¥å…·", layout="wide")
    
    st.title("ğŸ“Š é»‘é¾™æ±Ÿæ—¥æ¸…åˆ†ç»“ç®—å•æ•°æ®æå–å·¥å…·ï¼ˆåŒåœºç«™å¢å¼ºç‰ˆï¼‰")
    st.markdown("**æ ¸å¿ƒæ”¹è¿›ï¼šæ”¯æŒåŒåœºç«™(A/B)è¯†åˆ«ã€ä¿®å¤æ•°æ®æå–é”™è¯¯ã€å‡å°‘Noneå€¼**")
    st.divider()
    
    # æ˜¾ç¤ºç§‘ç›®ä¿¡æ¯
    with st.expander("ğŸ“‹ æ”¯æŒçš„ç§‘ç›®åˆ—è¡¨"):
        st.write("**å¸¸è§„ç§‘ç›®ï¼ˆç”µé‡ã€ç”µä»·ã€ç”µè´¹ï¼‰ï¼š**")
        for trade in REGULAR_TRADES:
            st.write(f"- {trade}")
        
        st.write("**ç‰¹æ®Šç§‘ç›®ï¼ˆä»…ç”µè´¹ï¼‰ï¼š**")
        for trade in SPECIAL_TRADES:
            st.write(f"- {trade}")
    
    st.subheader("ğŸ“ ä¸Šä¼ æ–‡ä»¶")
    uploaded_files = st.file_uploader(
        "æ”¯æŒPDFæ ¼å¼ï¼Œå¯æ‰¹é‡ä¸Šä¼ ï¼ˆæ”¯æŒåŒåœºç«™PDFï¼‰",
        type=['pdf'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        if st.button("ğŸš€ å¼€å§‹å¤„ç†", type="primary"):
            st.divider()
            st.subheader("âš™ï¸ å¤„ç†è¿›åº¦")
            
            all_data = []
            progress_bar = st.progress(0)
            
            for idx, file in enumerate(uploaded_files):
                progress_bar.progress((idx + 1) / len(uploaded_files))
                
                try:
                    # å¤„ç†PDFï¼ˆå¯èƒ½è¿”å›å¤šä¸ªåœºç«™çš„æ•°æ®ï¼‰
                    file_results = extract_data_from_pdf_v2(file, file.name)
                    for result in file_results:
                        all_data.append(result)
                    
                    # æ˜¾ç¤ºå¤„ç†ç»“æœ
                    if len(file_results) == 2:
                        st.success(f"âœ“ {file.name} å¤„ç†æˆåŠŸï¼ˆè¯†åˆ«å‡º2ä¸ªåœºç«™ï¼‰")
                    elif len(file_results) == 1:
                        st.success(f"âœ“ {file.name} å¤„ç†æˆåŠŸï¼ˆè¯†åˆ«å‡º1ä¸ªåœºç«™ï¼‰")
                    else:
                        st.warning(f"âš  {file.name} å¤„ç†å®Œæˆï¼Œä½†æœªè¯†åˆ«åˆ°åœºç«™æ•°æ®")
                        
                except Exception as e:
                    st.error(f"âœ— {file.name} å¤„ç†å¤±è´¥: {str(e)}")
            
            progress_bar.empty()
            
            if all_data:
                # æ„å»ºç»“æœDataFrame
                result_columns = ['åœºç«™åç§°', 'æ¸…åˆ†æ—¥æœŸ', 'åˆè®¡ç”µé‡(å…†ç“¦æ—¶)', 'åˆè®¡ç”µè´¹(å…ƒ)']
                
                for trade in REGULAR_TRADES:
                    # ç®€åŒ–åˆ—åï¼Œé¿å…è¿‡é•¿
                    trade_short = trade.replace('çœé—´ç»¿è‰²ç”µåŠ›äº¤æ˜“', 'çœé—´ç»¿ç”µäº¤æ˜“')
                    trade_short = trade_short.replace('ç”µç½‘ä¼ä¸šä»£ç†è´­ç”µäº¤æ˜“', 'ä»£ç†è´­ç”µäº¤æ˜“')
                    result_columns.extend([f'{trade_short}_ç”µé‡', f'{trade_short}_ç”µä»·', f'{trade_short}_ç”µè´¹'])
                
                for trade in SPECIAL_TRADES:
                    result_columns.append(f'{trade}_ç”µè´¹')
                
                result_df = pd.DataFrame(all_data, columns=result_columns)
                
                # æ˜¾ç¤ºç»“æœ
                st.subheader("ğŸ“ˆ æå–ç»“æœ")
                st.dataframe(result_df, use_container_width=True)
                
                # å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
                st.info(f"**ç»Ÿè®¡ä¿¡æ¯ï¼š** å…±å¤„ç† {len(all_data)} æ¡åœºç«™è®°å½•ï¼Œæ¶‰åŠ {result_df['åœºç«™åç§°'].nunique()} ä¸ªåœºç«™")
                
                # æ£€æŸ¥åŒå‘A/Bé£ç”µåœº
                has_a_station = any('åŒå‘A' in str(name) for name in result_df['åœºç«™åç§°'])
                has_b_station = any('åŒå‘B' in str(name) for name in result_df['åœºç«™åç§°'])
                
                if has_a_station and has_b_station:
                    st.success("âœ… æˆåŠŸè¯†åˆ«åŒå‘A/Bé£ç”µåœºæ•°æ®")
                elif has_a_station:
                    st.warning("âš ï¸ ä»…è¯†åˆ«åˆ°åŒå‘Aé£ç”µåœºï¼Œæœªæ£€æµ‹åˆ°Bé£ç”µåœºæ•°æ®")
                elif has_b_station:
                    st.warning("âš ï¸ ä»…è¯†åˆ«åˆ°åŒå‘Bé£ç”µåœºï¼Œæœªæ£€æµ‹åˆ°Aé£ç”µåœºæ•°æ®")
                
                # æ›´ç²¾å‡†çš„æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
                data_columns = result_columns[4:]
                non_null_count = result_df[data_columns].notna().sum()
                total_cells = len(result_df) * len(data_columns)
                filled_cells = result_df[data_columns].notna().sum().sum()
                
                st.info(f"**æ•°æ®å®Œæ•´æ€§ï¼š**")
                st.info(f"- æ€»æ•°æ®å•å…ƒæ ¼ï¼š{total_cells}")
                st.info(f"- æœ‰å€¼å•å…ƒæ ¼ï¼š{filled_cells} ({filled_cells/total_cells*100:.1f}%)")
                
                # ä¸‹è½½åŠŸèƒ½
                current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    result_df.to_excel(writer, index=False)
                output.seek(0)
                
                st.download_button(
                    label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
                    data=output,
                    file_name=f"é»‘é¾™æ±Ÿç»“ç®—æ•°æ®_åŒåœºç«™ç‰ˆ_{current_time}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    type="primary"
                )
                
                st.success("âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    
    else:
        st.info("ğŸ‘† è¯·ä¸Šä¼ PDFæ–‡ä»¶å¼€å§‹å¤„ç†ï¼ˆæ”¯æŒåŒ…å«åŒåœºç«™çš„PDFï¼‰")

if __name__ == "__main__":
    main()
